"""
Monitoring et Tracking de Performance - FreeMobilaChat
======================================================

Module de surveillance temps r√©el des m√©triques de performance,
qualit√© et business pour le syst√®me de classification.

Fonctionnalit√©s:
- Tracking latence et throughput
- Surveillance qualit√© des pr√©dictions
- D√©tection de drift des donn√©es
- Alertes automatiques
- Logs structur√©s pour analyse
"""

import time
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import deque
import statistics

import pandas as pd
import numpy as np
import psutil

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('monitoring/logs/performance.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class PerformanceTracker:
    """
    Tracker de performance pour monitoring temps r√©el
    
    Mesure et stocke les m√©triques cl√©s:
    - Latence classification (p50, p95, p99)
    - Throughput (tweets/seconde)
    - Utilisation ressources (CPU, RAM)
    - Taux d'erreur et fallback
    - Distribution de confiance
    """
    
    def __init__(self, window_size: int = 1000):
        """
        Initialise le tracker avec fen√™tre glissante
        
        Args:
            window_size: Taille de la fen√™tre pour calculs glissants
        """
        self.window_size = window_size
        
        # Fen√™tres glissantes pour m√©triques
        self.latencies = deque(maxlen=window_size)
        self.confidences = deque(maxlen=window_size)
        self.errors = deque(maxlen=window_size)
        self.fallbacks = deque(maxlen=window_size)
        
        # Compteurs globaux
        self.total_requests = 0
        self.total_errors = 0
        self.total_fallbacks = 0
        self.start_time = time.time()
        
        # Stockage des m√©triques
        self.metrics_history = []
        
        # Cr√©ation r√©pertoire logs
        Path("monitoring/logs").mkdir(parents=True, exist_ok=True)
        Path("monitoring/metrics").mkdir(parents=True, exist_ok=True)
        
        logger.info("PerformanceTracker initialis√© avec window_size=%d", window_size)
    
    def track_request(
        self,
        latency: float,
        confidence: float,
        error: bool = False,
        fallback: bool = False,
        metadata: Optional[Dict] = None
    ):
        """
        Enregistre une requ√™te de classification
        
        Args:
            latency: Temps de traitement en secondes
            confidence: Score de confiance [0-1]
            error: Indique si erreur s'est produite
            fallback: Indique si fallback utilis√©
            metadata: Donn√©es additionnelles (mod√®le, mode, etc.)
        """
        self.total_requests += 1
        
        # Mise √† jour fen√™tres glissantes
        self.latencies.append(latency)
        self.confidences.append(confidence)
        self.errors.append(1 if error else 0)
        self.fallbacks.append(1 if fallback else 0)
        
        # Compteurs globaux
        if error:
            self.total_errors += 1
        if fallback:
            self.total_fallbacks += 1
        
        # Log si latence anormale
        if latency > 5.0:
            logger.warning(
                "Latence √©lev√©e d√©tect√©e: %.2fs (requ√™te #%d)",
                latency,
                self.total_requests
            )
        
        # Log si erreur
        if error:
            logger.error(
                "Erreur classification (requ√™te #%d): %s",
                self.total_requests,
                metadata.get('error_message', 'Unknown') if metadata else 'Unknown'
            )
    
    def get_current_metrics(self) -> Dict[str, Any]:
        """
        R√©cup√®re les m√©triques actuelles
        
        Returns:
            Dictionnaire avec toutes les m√©triques courantes
        """
        uptime = time.time() - self.start_time
        
        # Calcul m√©triques latence
        latency_metrics = {}
        if self.latencies:
            latencies_sorted = sorted(self.latencies)
            latency_metrics = {
                'mean': statistics.mean(self.latencies),
                'median': statistics.median(self.latencies),
                'p95': np.percentile(latencies_sorted, 95),
                'p99': np.percentile(latencies_sorted, 99),
                'min': min(self.latencies),
                'max': max(self.latencies)
            }
        
        # Calcul m√©triques confiance
        confidence_metrics = {}
        if self.confidences:
            confidence_metrics = {
                'mean': statistics.mean(self.confidences),
                'median': statistics.median(self.confidences),
                'std': statistics.stdev(self.confidences) if len(self.confidences) > 1 else 0.0
            }
        
        # Taux d'erreur et fallback
        error_rate = (sum(self.errors) / len(self.errors) * 100) if self.errors else 0.0
        fallback_rate = (sum(self.fallbacks) / len(self.fallbacks) * 100) if self.fallbacks else 0.0
        
        # Throughput
        throughput = self.total_requests / uptime if uptime > 0 else 0.0
        
        # Utilisation ressources
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'uptime_seconds': uptime,
            'total_requests': self.total_requests,
            'throughput': throughput,
            'latency': latency_metrics,
            'confidence': confidence_metrics,
            'error_rate': error_rate,
            'fallback_rate': fallback_rate,
            'total_errors': self.total_errors,
            'total_fallbacks': self.total_fallbacks,
            'resources': {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_available_mb': memory.available / (1024 ** 2)
            }
        }
        
        return metrics
    
    def save_metrics(self, filepath: Optional[str] = None):
        """
        Sauvegarde les m√©triques actuelles dans un fichier JSON
        
        Args:
            filepath: Chemin du fichier (auto-g√©n√©r√© si None)
        """
        metrics = self.get_current_metrics()
        
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = f"monitoring/metrics/metrics_{timestamp}.json"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        logger.info("M√©triques sauvegard√©es: %s", filepath)
        
        # Ajouter √† l'historique
        self.metrics_history.append(metrics)
    
    def check_alerts(self) -> List[Dict[str, str]]:
        """
        V√©rifie si des seuils d'alerte sont d√©pass√©s
        
        Returns:
            Liste des alertes actives
        """
        alerts = []
        metrics = self.get_current_metrics()
        
        # Alerte latence √©lev√©e (p95 > 4s)
        if metrics['latency'].get('p95', 0) > 4.0:
            alerts.append({
                'severity': 'warning',
                'metric': 'latency_p95',
                'value': metrics['latency']['p95'],
                'threshold': 4.0,
                'message': f"Latence P95 √©lev√©e: {metrics['latency']['p95']:.2f}s"
            })
        
        # Alerte taux d'erreur √©lev√© (>5%)
        if metrics['error_rate'] > 5.0:
            alerts.append({
                'severity': 'critical',
                'metric': 'error_rate',
                'value': metrics['error_rate'],
                'threshold': 5.0,
                'message': f"Taux d'erreur critique: {metrics['error_rate']:.1f}%"
            })
        
        # Alerte fallback excessif (>20%)
        if metrics['fallback_rate'] > 20.0:
            alerts.append({
                'severity': 'warning',
                'metric': 'fallback_rate',
                'value': metrics['fallback_rate'],
                'threshold': 20.0,
                'message': f"Taux fallback √©lev√©: {metrics['fallback_rate']:.1f}%"
            })
        
        # Alerte m√©moire critique (<500MB disponible)
        if metrics['resources']['memory_available_mb'] < 500:
            alerts.append({
                'severity': 'critical',
                'metric': 'memory_available',
                'value': metrics['resources']['memory_available_mb'],
                'threshold': 500,
                'message': f"M√©moire faible: {metrics['resources']['memory_available_mb']:.0f}MB"
            })
        
        # Alerte CPU √©lev√© (>90%)
        if metrics['resources']['cpu_percent'] > 90:
            alerts.append({
                'severity': 'warning',
                'metric': 'cpu_percent',
                'value': metrics['resources']['cpu_percent'],
                'threshold': 90,
                'message': f"CPU √©lev√©: {metrics['resources']['cpu_percent']:.1f}%"
            })
        
        # Log les alertes
        for alert in alerts:
            if alert['severity'] == 'critical':
                logger.critical("ALERTE: %s", alert['message'])
            else:
                logger.warning("ALERTE: %s", alert['message'])
        
        return alerts
    
    def generate_report(self) -> str:
        """
        G√©n√®re un rapport de performance format√©
        
        Returns:
            Rapport texte multi-lignes
        """
        metrics = self.get_current_metrics()
        
        report = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë          RAPPORT DE PERFORMANCE - FREEMOBILACHAT             ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Timestamp: {metrics['timestamp']}                    
‚ïë Uptime: {metrics['uptime_seconds'] / 3600:.1f}h                                            
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë TRAFIC                                                       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Total requ√™tes: {metrics['total_requests']:,}                                  
‚ïë Throughput: {metrics['throughput']:.2f} req/s                               
‚ïë Taux erreur: {metrics['error_rate']:.2f}%                                    
‚ïë Taux fallback: {metrics['fallback_rate']:.2f}%                                  
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë LATENCE                                                      ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Moyenne: {metrics['latency'].get('mean', 0):.3f}s                                         
‚ïë M√©diane: {metrics['latency'].get('median', 0):.3f}s                                         
‚ïë P95: {metrics['latency'].get('p95', 0):.3f}s                                             
‚ïë P99: {metrics['latency'].get('p99', 0):.3f}s                                             
‚ïë Min/Max: {metrics['latency'].get('min', 0):.3f}s / {metrics['latency'].get('max', 0):.3f}s                              
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë CONFIANCE                                                    ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Moyenne: {metrics['confidence'].get('mean', 0):.3f}                                          
‚ïë M√©diane: {metrics['confidence'].get('median', 0):.3f}                                          
‚ïë √âcart-type: {metrics['confidence'].get('std', 0):.3f}                                       
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë RESSOURCES                                                   ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë CPU: {metrics['resources']['cpu_percent']:.1f}%                                                   
‚ïë M√©moire: {metrics['resources']['memory_percent']:.1f}% ({metrics['resources']['memory_available_mb']:.0f}MB dispo)            
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
        return report
    
    def reset(self):
        """R√©initialise tous les compteurs et m√©triques"""
        self.latencies.clear()
        self.confidences.clear()
        self.errors.clear()
        self.fallbacks.clear()
        self.total_requests = 0
        self.total_errors = 0
        self.total_fallbacks = 0
        self.start_time = time.time()
        logger.info("PerformanceTracker r√©initialis√©")


class DataDriftDetector:
    """
    D√©tecteur de drift des donn√©es
    
    Surveille les changements dans la distribution des donn√©es
    pour d√©tecter une d√©gradation potentielle du mod√®le
    """
    
    def __init__(self, baseline_data: pd.DataFrame):
        """
        Initialise le d√©tecteur avec donn√©es de r√©f√©rence
        
        Args:
            baseline_data: DataFrame de r√©f√©rence pour comparaison
        """
        self.baseline_stats = self._compute_statistics(baseline_data)
        logger.info("DataDriftDetector initialis√© avec %d √©chantillons baseline", len(baseline_data))
    
    def _compute_statistics(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Calcule statistiques de distribution"""
        stats = {}
        
        # Longueur moyenne des tweets
        if 'text_length' in data.columns:
            stats['text_length'] = {
                'mean': data['text_length'].mean(),
                'std': data['text_length'].std(),
                'median': data['text_length'].median()
            }
        
        # Distribution des sentiments
        if 'sentiment' in data.columns:
            stats['sentiment_dist'] = data['sentiment'].value_counts(normalize=True).to_dict()
        
        # Distribution des intentions
        if 'intention' in data.columns:
            stats['intention_dist'] = data['intention'].value_counts(normalize=True).to_dict()
        
        return stats
    
    def detect_drift(self, current_data: pd.DataFrame, threshold: float = 0.15) -> Dict[str, Any]:
        """
        D√©tecte le drift par rapport au baseline
        
        Args:
            current_data: Donn√©es actuelles √† comparer
            threshold: Seuil de diff√©rence acceptable (15% par d√©faut)
            
        Returns:
            Rapport de drift avec alertes
        """
        current_stats = self._compute_statistics(current_data)
        drift_report = {'drift_detected': False, 'details': []}
        
        # V√©rifier drift longueur texte
        if 'text_length' in current_stats and 'text_length' in self.baseline_stats:
            baseline_mean = self.baseline_stats['text_length']['mean']
            current_mean = current_stats['text_length']['mean']
            diff_pct = abs(current_mean - baseline_mean) / baseline_mean
            
            if diff_pct > threshold:
                drift_report['drift_detected'] = True
                drift_report['details'].append({
                    'feature': 'text_length',
                    'baseline': baseline_mean,
                    'current': current_mean,
                    'diff_pct': diff_pct * 100
                })
        
        # V√©rifier drift distribution sentiments
        if 'sentiment_dist' in current_stats and 'sentiment_dist' in self.baseline_stats:
            for sentiment in self.baseline_stats['sentiment_dist']:
                baseline_pct = self.baseline_stats['sentiment_dist'].get(sentiment, 0)
                current_pct = current_stats['sentiment_dist'].get(sentiment, 0)
                diff = abs(current_pct - baseline_pct)
                
                if diff > threshold:
                    drift_report['drift_detected'] = True
                    drift_report['details'].append({
                        'feature': f'sentiment_{sentiment}',
                        'baseline': baseline_pct,
                        'current': current_pct,
                        'diff_pct': diff * 100
                    })
        
        if drift_report['drift_detected']:
            logger.warning("DRIFT D√âTECT√â: %d anomalies", len(drift_report['details']))
        
        return drift_report


# Fonction utilitaire pour int√©gration Streamlit
def create_streamlit_dashboard(tracker: PerformanceTracker):
    """
    Cr√©e un dashboard Streamlit avec m√©triques temps r√©el
    
    Args:
        tracker: Instance de PerformanceTracker
    """
    import streamlit as st
    
    st.header("üìä Performance Monitoring")
    
    metrics = tracker.get_current_metrics()
    
    # M√©triques principales
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Total Requ√™tes",
            f"{metrics['total_requests']:,}",
            f"{metrics['throughput']:.1f} req/s"
        )
    
    with col2:
        st.metric(
            "Latence P95",
            f"{metrics['latency'].get('p95', 0):.2f}s",
            delta=None
        )
    
    with col3:
        st.metric(
            "Taux Erreur",
            f"{metrics['error_rate']:.1f}%",
            delta=-metrics['error_rate'] if metrics['error_rate'] < 5 else metrics['error_rate'],
            delta_color="inverse"
        )
    
    with col4:
        st.metric(
            "Confiance Moy.",
            f"{metrics['confidence'].get('mean', 0):.2f}",
            delta=None
        )
    
    # Alertes
    alerts = tracker.check_alerts()
    if alerts:
        st.warning(f"‚ö†Ô∏è {len(alerts)} alertes actives")
        for alert in alerts:
            st.error(f"**{alert['metric']}**: {alert['message']}")
    else:
        st.success("‚úÖ Aucune alerte")
    
    # Rapport d√©taill√©
    with st.expander("üìã Rapport D√©taill√©"):
        st.code(tracker.generate_report())


if __name__ == "__main__":
    # Test du tracker
    tracker = PerformanceTracker(window_size=100)
    
    # Simulation de requ√™tes
    import random
    for i in range(50):
        tracker.track_request(
            latency=random.uniform(0.5, 3.0),
            confidence=random.uniform(0.7, 0.95),
            error=random.random() < 0.02,
            fallback=random.random() < 0.1
        )
    
    # Affichage rapport
    print(tracker.generate_report())
    
    # V√©rification alertes
    alerts = tracker.check_alerts()
    print(f"\n{len(alerts)} alertes d√©tect√©es")
    
    # Sauvegarde m√©triques
    tracker.save_metrics()
