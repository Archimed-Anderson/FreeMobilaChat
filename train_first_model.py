"""
üöÄ √âTAPE 1: ENTRA√éNEMENT DU PREMIER MOD√àLE
==========================================
Entra√Ænement d'un mod√®le baseline simple sur le dataset d'entra√Ænement

Approche: Classification Multi-T√¢che avec TF-IDF + Logistic Regression
- Simple et rapide
- Baseline pour comparaisons futures
- Pas de d√©pendances GPU

Date: 2025-11-08
"""

import sys
import os
sys.path.insert(0, 'streamlit_app')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import pickle
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("\n" + "‚ïî" + "="*78 + "‚ïó")
print("‚ïë" + " "*20 + "üöÄ √âTAPE 1: ENTRA√éNEMENT PREMIER MOD√àLE" + " "*18 + "‚ïë")
print("‚ïë" + " "*25 + "Mod√®le Baseline Multi-T√¢che" + " "*24 + "‚ïë")
print("‚ïö" + "="*78 + "‚ïù\n")

# ============================================================================
# CONFIGURATION
# ============================================================================
CONFIG = {
    'data_file': 'data/training/train_dataset.csv',
    'output_dir': 'models/baseline',
    'test_size': 0.2,
    'random_state': 42,
    'max_features': 5000,
    'ngram_range': (1, 2)
}

# Cr√©er le dossier de sortie
os.makedirs(CONFIG['output_dir'], exist_ok=True)

# ============================================================================
# PHASE 1: CHARGEMENT DES DONN√âES
# ============================================================================
print("üìÇ [1/6] Chargement du dataset...")
df = pd.read_csv(CONFIG['data_file'])
print(f"   ‚úÖ {len(df):,} tweets charg√©s")
print(f"   Colonnes: {list(df.columns)}\n")

# ============================================================================
# PHASE 2: PR√âPARATION DES DONN√âES
# ============================================================================
print("üîß [2/6] Pr√©paration des donn√©es pour l'entra√Ænement...")

# Features (X)
X = df['text_cleaned'].fillna('')

# Labels (y) - Multiple outputs
y_sentiment = df['sentiment']
y_categorie = df['cat√©gorie']
y_priority = df['priority']

print(f"   ‚úÖ Features pr√©par√©es: {len(X):,} √©chantillons")
print(f"   ‚úÖ Labels sentiment: {y_sentiment.nunique()} classes")
print(f"   ‚úÖ Labels cat√©gorie: {y_categorie.nunique()} classes")
print(f"   ‚úÖ Labels priorit√©: {y_priority.nunique()} classes\n")

# ============================================================================
# PHASE 3: SPLIT TRAIN/TEST
# ============================================================================
print(f"üìä [3/6] Split des donn√©es (train {100-CONFIG['test_size']*100:.0f}% / test {CONFIG['test_size']*100:.0f}%)...")

X_train, X_test, y_sent_train, y_sent_test = train_test_split(
    X, y_sentiment, test_size=CONFIG['test_size'], random_state=CONFIG['random_state'], stratify=y_sentiment
)

_, _, y_cat_train, y_cat_test = train_test_split(
    X, y_categorie, test_size=CONFIG['test_size'], random_state=CONFIG['random_state'], stratify=y_sentiment
)

_, _, y_pri_train, y_pri_test = train_test_split(
    X, y_priority, test_size=CONFIG['test_size'], random_state=CONFIG['random_state'], stratify=y_sentiment
)

print(f"   ‚úÖ Train: {len(X_train):,} √©chantillons")
print(f"   ‚úÖ Test:  {len(X_test):,} √©chantillons\n")

# ============================================================================
# PHASE 4: VECTORISATION (TF-IDF)
# ============================================================================
print("üî§ [4/6] Vectorisation TF-IDF...")

vectorizer = TfidfVectorizer(
    max_features=CONFIG['max_features'],
    ngram_range=CONFIG['ngram_range'],
    min_df=2,
    max_df=0.95,
    stop_words=None  # Pas de stop words pour le fran√ßais
)

print(f"   Configuration:")
print(f"   ‚Ä¢ Max features: {CONFIG['max_features']}")
print(f"   ‚Ä¢ N-grams: {CONFIG['ngram_range']}")
print(f"   ‚Ä¢ Stop words: None (fran√ßais)")

X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

print(f"   ‚úÖ Matrice TF-IDF cr√©√©e: {X_train_tfidf.shape}\n")

# ============================================================================
# PHASE 5: ENTRA√éNEMENT DES MOD√àLES
# ============================================================================
print("ü§ñ [5/6] Entra√Ænement des classificateurs...")

# Mod√®le 1: Sentiment
print("   [1/3] Entra√Ænement du classificateur de SENTIMENT...")
model_sentiment = LogisticRegression(
    max_iter=1000,
    random_state=CONFIG['random_state'],
    class_weight='balanced'
)
model_sentiment.fit(X_train_tfidf, y_sent_train)
print(f"   ‚úÖ Mod√®le sentiment entra√Æn√©")

# Mod√®le 2: Cat√©gorie
print("   [2/3] Entra√Ænement du classificateur de CAT√âGORIE...")
model_categorie = LogisticRegression(
    max_iter=1000,
    random_state=CONFIG['random_state'],
    class_weight='balanced'
)
model_categorie.fit(X_train_tfidf, y_cat_train)
print(f"   ‚úÖ Mod√®le cat√©gorie entra√Æn√©")

# Mod√®le 3: Priorit√©
print("   [3/3] Entra√Ænement du classificateur de PRIORIT√â...")
model_priority = LogisticRegression(
    max_iter=1000,
    random_state=CONFIG['random_state'],
    class_weight='balanced'
)
model_priority.fit(X_train_tfidf, y_pri_train)
print(f"   ‚úÖ Mod√®le priorit√© entra√Æn√©\n")

# ============================================================================
# PHASE 6: √âVALUATION
# ============================================================================
print("üìä [6/6] √âvaluation des mod√®les...")

# Pr√©dictions
y_sent_pred = model_sentiment.predict(X_test_tfidf)
y_cat_pred = model_categorie.predict(X_test_tfidf)
y_pri_pred = model_priority.predict(X_test_tfidf)

# M√©triques Sentiment
print("\n" + "="*80)
print("üìä R√âSULTATS - SENTIMENT")
print("="*80)
acc_sent = accuracy_score(y_sent_test, y_sent_pred)
print(f"\n‚úÖ Accuracy: {acc_sent:.4f} ({acc_sent*100:.2f}%)\n")
print("Classification Report:")
print(classification_report(y_sent_test, y_sent_pred, zero_division=0))

# M√©triques Cat√©gorie
print("\n" + "="*80)
print("üìä R√âSULTATS - CAT√âGORIE")
print("="*80)
acc_cat = accuracy_score(y_cat_test, y_cat_pred)
print(f"\n‚úÖ Accuracy: {acc_cat:.4f} ({acc_cat*100:.2f}%)\n")
print("Classification Report:")
print(classification_report(y_cat_test, y_cat_pred, zero_division=0))

# M√©triques Priorit√©
print("\n" + "="*80)
print("üìä R√âSULTATS - PRIORIT√â")
print("="*80)
acc_pri = accuracy_score(y_pri_test, y_pri_pred)
print(f"\n‚úÖ Accuracy: {acc_pri:.4f} ({acc_pri*100:.2f}%)\n")
print("Classification Report:")
print(classification_report(y_pri_test, y_pri_pred, zero_division=0))

# ============================================================================
# SAUVEGARDE DES MOD√àLES
# ============================================================================
print("\n" + "="*80)
print("üíæ SAUVEGARDE DES MOD√àLES")
print("="*80 + "\n")

# Sauvegarder les mod√®les
models = {
    'vectorizer': vectorizer,
    'sentiment': model_sentiment,
    'categorie': model_categorie,
    'priority': model_priority
}

for name, model in models.items():
    filepath = os.path.join(CONFIG['output_dir'], f'{name}_model.pkl')
    with open(filepath, 'wb') as f:
        pickle.dump(model, f)
    print(f"   ‚úÖ {name.capitalize()} sauvegard√©: {filepath}")

# Sauvegarder les m√©triques
metrics = {
    'date': datetime.now().isoformat(),
    'dataset_size': len(df),
    'train_size': len(X_train),
    'test_size': len(X_test),
    'config': CONFIG,
    'results': {
        'sentiment': {
            'accuracy': float(acc_sent),
            'classes': list(y_sentiment.unique())
        },
        'categorie': {
            'accuracy': float(acc_cat),
            'classes': list(y_categorie.unique())
        },
        'priority': {
            'accuracy': float(acc_pri),
            'classes': list(y_priority.unique())
        }
    }
}

metrics_file = os.path.join(CONFIG['output_dir'], 'training_metrics.json')
with open(metrics_file, 'w', encoding='utf-8') as f:
    json.dump(metrics, f, indent=2, ensure_ascii=False)

print(f"   ‚úÖ M√©triques sauvegard√©es: {metrics_file}")

# ============================================================================
# R√âSUM√â FINAL
# ============================================================================
print("\n" + "‚ïî" + "="*78 + "‚ïó")
print("‚ïë" + " "*25 + "‚úÖ ENTRA√éNEMENT R√âUSSI!" + " "*26 + "‚ïë")
print("‚ïö" + "="*78 + "‚ïù\n")

print("üìä R√âSUM√â DES PERFORMANCES:\n")
print(f"   ‚Ä¢ Sentiment:  {acc_sent*100:.2f}% accuracy")
print(f"   ‚Ä¢ Cat√©gorie:  {acc_cat*100:.2f}% accuracy")
print(f"   ‚Ä¢ Priorit√©:   {acc_pri*100:.2f}% accuracy")
print(f"\n   Moyenne:     {(acc_sent + acc_cat + acc_pri)/3*100:.2f}% accuracy")

print(f"\nüìÅ MOD√àLES SAUVEGARD√âS:")
print(f"   ‚Ä¢ Dossier: {CONFIG['output_dir']}/")
print(f"   ‚Ä¢ Fichiers: 4 mod√®les + m√©triques")

print(f"\nüéØ MOD√àLE BASELINE CR√â√â:")
print(f"   ‚úÖ Pr√™t pour comparaisons futures")
print(f"   ‚úÖ Peut √™tre utilis√© pour inf√©rence")
print(f"   ‚úÖ Benchmark √©tabli pour fine-tuning")

print("\n" + "="*80)
print("  üéâ √âTAPE 1 COMPL√âT√âE AVEC SUCC√àS!")
print("="*80 + "\n")

print("üìñ PROCHAINE √âTAPE:")
print("   ‚Üí √âtape 2: G√©n√©rer les datasets de validation et test\n")

