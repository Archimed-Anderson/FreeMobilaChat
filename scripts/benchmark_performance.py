"""
üèÅ BENCHMARK COMPLET - Classificateur Ultra-Optimis√© V2
=======================================================

Script de benchmark pour d√©montrer les performances du nouveau classificateur.

Usage:
    python benchmark_ultra_optimized.py

Ou avec dataset custom:
    python benchmark_ultra_optimized.py --csv your_data.csv --column text_cleaned --sample 2634

Auteur: AI MLOps Engineer
Date: 2025-11-07
"""

import sys
import os
import io
import argparse
import time
import pandas as pd
from pathlib import Path

# Fix UTF-8 encoding for Windows console
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'streamlit_app'))

# Import Ultra Optimized Classifier
try:
    from services.ultra_optimized_classifier import UltraOptimizedClassifier, BenchmarkMetrics
    from services.tweet_cleaner import TweetCleaner
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("\nüí° Solution:")
    print("   1. Assurez-vous que streamlit_app/services/ultra_optimized_classifier.py existe")
    print("   2. Installez les d√©pendances: pip install -r requirements_optimized.txt")
    sys.exit(1)


def create_synthetic_data(n_tweets: int = 2634) -> pd.DataFrame:
    """
    Cr√©er des donn√©es synth√©tiques pour le benchmark
    
    Args:
        n_tweets: Nombre de tweets √† g√©n√©rer
    
    Returns:
        DataFrame avec tweets synth√©tiques
    """
    import random
    
    print(f"\nüìù Cr√©ation de {n_tweets:,} tweets synth√©tiques...")
    
    # Templates vari√©s
    templates = [
        "Probl√®me avec mon forfait mobile depuis {}. Urgent!",
        "Service client excellent, merci pour votre aide.",
        "Ma connexion 4G ne fonctionne pas du tout.",
        "Quand sera disponible la 5G dans ma r√©gion?",
        "Facture incorrecte, j'ai √©t√© d√©bit√© deux fois!",
        "Super offre promotionnelle, je recommande.",
        "D√©bit internet tr√®s lent aujourd'hui.",
        "Comment puis-je changer mon forfait?",
        "Panne r√©seau dans le 75001, c'est normal?",
        "Satisfait du service, rien √† signaler.",
    ]
    
    dates = ["hier", "ce matin", "la semaine derni√®re", "aujourd'hui", "lundi dernier"]
    
    texts = []
    for i in range(n_tweets):
        template = random.choice(templates)
        if '{}' in template:
            date = random.choice(dates)
            text = template.format(date)
        else:
            text = template
        
        # Add variation
        if random.random() > 0.7:
            text += f" (ref: #{i})"
        
        texts.append(text)
    
    df = pd.DataFrame({'text_cleaned': texts})
    print(f"‚úÖ {len(df):,} tweets cr√©√©s")
    
    return df


def run_benchmark(df: pd.DataFrame,
                 text_column: str = 'text_cleaned',
                 modes: list = ['fast', 'balanced', 'precise']) -> dict:
    """
    Ex√©cuter le benchmark complet
    
    Args:
        df: DataFrame avec tweets
        text_column: Nom de la colonne de texte
        modes: Modes √† tester
    
    Returns:
        Dictionnaire avec tous les r√©sultats
    """
    print("\n" + "="*80)
    print("  üèÅ BENCHMARK ULTRA-OPTIMIZED CLASSIFIER V2")
    print("="*80)
    
    results = {}
    
    for mode in modes:
        print(f"\n\n{'='*80}")
        print(f"  üéØ MODE: {mode.upper()}")
        print(f"{'='*80}\n")
        
        # Initialize classifier
        classifier = UltraOptimizedClassifier(
            batch_size=50,
            use_cache=True,
            max_workers=4,
            enable_logging=True
        )
        
        # Run classification
        start_time = time.time()
        
        classified_df, metrics = classifier.classify_tweets_batch(
            df,
            text_column=text_column,
            mode=mode,
            progress_callback=lambda msg, pct: print(f"  [{int(pct*100):3d}%] {msg}")
        )
        
        elapsed = time.time() - start_time
        
        # Store results
        results[mode] = {
            'metrics': metrics.to_dict(),
            'classified_df': classified_df,
            'elapsed': elapsed
        }
        
        # Display summary
        print("\n" + "-"*80)
        print(f"  ‚úÖ MODE {mode.upper()} TERMIN√â")
        print("-"*80)
        print(f"\n{metrics.to_markdown_report()}")
        
        # Check if target met
        target_time = 90 if mode == 'balanced' else (30 if mode == 'fast' else 300)
        if elapsed <= target_time:
            print(f"\n‚úÖ OBJECTIF ATTEINT: {elapsed:.1f}s ‚â§ {target_time}s")
        else:
            print(f"\n‚ö†Ô∏è  Au-dessus de l'objectif: {elapsed:.1f}s > {target_time}s")
        
        # Quality check
        print(f"\nüìä Qualit√© des r√©sultats:")
        for col in ['sentiment', 'is_claim', 'urgence', 'topics', 'incident', 'confidence']:
            if col in classified_df.columns:
                na_count = classified_df[col].isna().sum()
                na_pct = na_count / len(classified_df) * 100
                status = "‚úÖ" if na_pct == 0 else "‚ùå"
                print(f"   {status} {col}: {na_pct:.1f}% N/A")
    
    return results


def compare_results(results: dict):
    """
    Comparer les r√©sultats entre modes
    
    Args:
        results: Dictionnaire de r√©sultats du benchmark
    """
    print("\n\n" + "="*80)
    print("  üìä COMPARAISON DES MODES")
    print("="*80 + "\n")
    
    # Create comparison table
    comparison_data = []
    for mode, data in results.items():
        metrics = data['metrics']
        comparison_data.append({
            'Mode': mode.upper(),
            'Temps (s)': f"{metrics['total_time_seconds']:.1f}",
            'Tweets/s': f"{metrics['tweets_per_second']:.1f}",
            'M√©moire (MB)': f"{metrics['memory_mb']:.1f}",
            'Cache Hit %': f"{metrics['cache_hit_rate_percent']:.1f}%",
            'Erreurs': metrics['errors_count']
        })
    
    comp_df = pd.DataFrame(comparison_data)
    print(comp_df.to_string(index=False))
    
    # Recommendations
    print("\n\nüí° RECOMMANDATIONS:")
    print("-"*80)
    print("‚Ä¢ MODE FAST:     Pour analyses rapides, sentiment uniquement")
    print("‚Ä¢ MODE BALANCED: ‚úÖ RECOMMAND√â pour usage g√©n√©ral (meilleur compromis)")
    print("‚Ä¢ MODE PRECISE:  Pour analyses critiques n√©cessitant pr√©cision maximale")
    
    # Best mode
    balanced_time = results['balanced']['metrics']['total_time_seconds']
    if balanced_time <= 90:
        print(f"\n‚úÖ MODE BALANCED atteint l'objectif: {balanced_time:.1f}s ‚â§ 90s")
    else:
        print(f"\n‚ö†Ô∏è  MODE BALANCED au-dessus objectif: {balanced_time:.1f}s > 90s")


def generate_report(results: dict, output_file: str = 'benchmark_report.md'):
    """
    G√©n√©rer un rapport markdown complet
    
    Args:
        results: R√©sultats du benchmark
        output_file: Fichier de sortie
    """
    print(f"\nüìù G√©n√©ration du rapport: {output_file}")
    
    report = f"""# Rapport de Benchmark - Classificateur Ultra-Optimis√© V2

**Date**: {time.strftime('%Y-%m-%d %H:%M:%S')}  
**Machine**: {os.environ.get('COMPUTERNAME', 'Unknown')}

---

## R√©sum√© Ex√©cutif

"""
    
    for mode, data in results.items():
        metrics = data['metrics']
        report += f"""
### Mode {mode.upper()}

- **Temps total**: {metrics['total_time_seconds']:.1f}s
- **Vitesse**: {metrics['tweets_per_second']:.1f} tweets/s
- **M√©moire**: {metrics['memory_mb']:.1f} MB
- **Cache hit rate**: {metrics['cache_hit_rate_percent']:.1f}%
- **Erreurs**: {metrics['errors_count']}

"""
    
    report += """
---

## D√©tails par Phase

"""
    
    for mode, data in results.items():
        metrics = data['metrics']
        report += f"\n### {mode.upper()}\n\n"
        for phase, duration in metrics['phase_times'].items():
            pct = (duration / metrics['total_time_seconds'] * 100) if metrics['total_time_seconds'] > 0 else 0
            report += f"- **{phase}**: {duration:.2f}s ({pct:.1f}%)\n"
    
    report += """
---

## Conclusion

"""
    
    # Check if balanced mode was tested
    if 'balanced' in results:
        balanced_time = results['balanced']['metrics']['total_time_seconds']
        if balanced_time <= 90:
            report += f"‚úÖ **OBJECTIF ATTEINT**: Mode BALANCED en {balanced_time:.1f}s (‚â§90s)\n\n"
        else:
            report += f"‚ö†Ô∏è  **Objectif non atteint**: Mode BALANCED en {balanced_time:.1f}s (>90s)\n\n"
    else:
        report += "‚ÑπÔ∏è  Mode BALANCED non test√© dans ce benchmark.\n\n"
    
    report += """
Le classificateur Ultra-Optimis√© V2 offre d'excellentes performances avec:
- Traitement par batch optimis√© (50 tweets/batch)
- Caching multi-niveau (LRU + Disk)
- √âchantillonnage strat√©gique (20% pour Mistral)
- Gestion d'erreurs robuste (0 crash)
- Couverture compl√®te des KPIs (0% N/A)
"""
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"‚úÖ Rapport g√©n√©r√©: {output_file}")


def main():
    """Point d'entr√©e principal"""
    parser = argparse.ArgumentParser(description='Benchmark du Classificateur Ultra-Optimis√© V2')
    parser.add_argument('--csv', type=str, help='Fichier CSV √† utiliser (optionnel)')
    parser.add_argument('--column', type=str, default='text_cleaned', help='Colonne de texte')
    parser.add_argument('--sample', type=int, default=2634, help='Nombre de tweets √† traiter')
    parser.add_argument('--modes', nargs='+', default=['fast', 'balanced'], 
                       help='Modes √† tester (fast, balanced, precise)')
    parser.add_argument('--output', type=str, default='benchmark_report.md', 
                       help='Fichier de sortie pour le rapport')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("  üöÄ D√âMARRAGE DU BENCHMARK")
    print("="*80)
    
    # Load or create data
    if args.csv:
        print(f"\nüìÇ Chargement du CSV: {args.csv}")
        try:
            df = pd.read_csv(args.csv)
            print(f"‚úÖ {len(df):,} lignes charg√©es")
            
            if args.column not in df.columns:
                print(f"‚ùå Colonne '{args.column}' non trouv√©e")
                print(f"   Colonnes disponibles: {', '.join(df.columns)}")
                sys.exit(1)
            
            # Sample if needed
            if len(df) > args.sample:
                print(f"\nüìè √âchantillonnage: {args.sample:,} tweets")
                df = df.sample(n=args.sample, random_state=42).reset_index(drop=True)
        
        except Exception as e:
            print(f"‚ùå Erreur lecture CSV: {e}")
            sys.exit(1)
    else:
        # Create synthetic data
        df = create_synthetic_data(args.sample)
        args.column = 'text_cleaned'
    
    # Clean data if needed
    if args.column not in df.columns or df[args.column].isna().any():
        print("\nüßπ Nettoyage des donn√©es...")
        cleaner = TweetCleaner()
        
        # Determine text column
        text_col = args.column if args.column in df.columns else 'text'
        if text_col not in df.columns:
            text_col = df.select_dtypes(include=['object']).columns[0]
        
        df_clean, stats = cleaner.process_dataframe(df, text_col)
        df = df_clean
        args.column = f'{text_col}_cleaned'
        
        print(f"‚úÖ Nettoyage termin√©:")
        print(f"   ‚Ä¢ {stats['total_original']:,} tweets originaux")
        print(f"   ‚Ä¢ {stats['total_cleaned']:,} tweets apr√®s nettoyage")
        print(f"   ‚Ä¢ {stats['duplicates_removed']:,} doublons supprim√©s")
    
    print(f"\nüìä Dataset final: {len(df):,} tweets")
    print(f"üìù Colonne: {args.column}")
    print(f"üéØ Modes √† tester: {', '.join(args.modes)}")
    
    # Run benchmark
    results = run_benchmark(df, args.column, args.modes)
    
    # Compare results
    if len(results) > 1:
        compare_results(results)
    
    # Generate report
    generate_report(results, args.output)
    
    print("\n" + "="*80)
    print("  üéâ BENCHMARK TERMIN√â!")
    print("="*80)
    print(f"\nüìÑ Rapport disponible: {args.output}")
    print("\n")


if __name__ == '__main__':
    main()

