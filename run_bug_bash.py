"""
ğŸ› Ã‰TAPE 4: SESSION DE BUG BASH
================================
ExÃ©cution systÃ©matique des tests et documentation des problÃ¨mes

Tests effectuÃ©s:
- ExÃ©cution de tous les scÃ©narios de test
- DÃ©tection des bugs et problÃ¨mes
- Documentation complÃ¨te des issues
- GÃ©nÃ©ration d'un rapport de bug bash

Date: 2025-11-08
"""

import sys
import os
sys.path.insert(0, 'streamlit_app')

import pandas as pd
import numpy as np
import pickle
import json
import time
import traceback
from datetime import datetime
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

print("\n" + "â•”" + "="*78 + "â•—")
print("â•‘" + " "*25 + "ğŸ› Ã‰TAPE 4: BUG BASH SESSION" + " "*24 + "â•‘")
print("â•‘" + " "*20 + "Tests SystÃ©matiques et Documentation" + " "*19 + "â•‘")
print("â•š" + "="*78 + "â•\n")

# ============================================================================
# CONFIGURATION
# ============================================================================
CONFIG = {
    'models_dir': 'models/baseline',
    'test_scenarios_file': 'tests/scenarios/test_scenarios.json',
    'test_cases_file': 'tests/scenarios/test_cases.json',
    'validation_file': 'data/training/validation_dataset.csv',
    'test_file': 'data/training/test_dataset_split.csv',
    'output_dir': 'tests/bug_bash_results'
}

os.makedirs(CONFIG['output_dir'], exist_ok=True)

print("âš™ï¸  CONFIGURATION:")
print(f"   â€¢ ModÃ¨les: {CONFIG['models_dir']}/")
print(f"   â€¢ ScÃ©narios: {CONFIG['test_scenarios_file']}")
print(f"   â€¢ RÃ©sultats: {CONFIG['output_dir']}/\n")

# ============================================================================
# PHASE 1: CHARGEMENT DES MODÃˆLES
# ============================================================================
print("ğŸ“¦ [1/6] Chargement des modÃ¨les...")

try:
    with open(os.path.join(CONFIG['models_dir'], 'vectorizer_model.pkl'), 'rb') as f:
        vectorizer = pickle.load(f)
    with open(os.path.join(CONFIG['models_dir'], 'sentiment_model.pkl'), 'rb') as f:
        model_sentiment = pickle.load(f)
    with open(os.path.join(CONFIG['models_dir'], 'categorie_model.pkl'), 'rb') as f:
        model_categorie = pickle.load(f)
    with open(os.path.join(CONFIG['models_dir'], 'priority_model.pkl'), 'rb') as f:
        model_priority = pickle.load(f)
    
    print("   âœ… Tous les modÃ¨les chargÃ©s avec succÃ¨s\n")
except Exception as e:
    print(f"   âŒ ERREUR lors du chargement: {e}\n")
    sys.exit(1)

# ============================================================================
# PHASE 2: CHARGEMENT DES SCÃ‰NARIOS ET CAS DE TEST
# ============================================================================
print("ğŸ“‹ [2/6] Chargement des scÃ©narios et cas de test...")

with open(CONFIG['test_scenarios_file'], 'r', encoding='utf-8') as f:
    test_scenarios = json.load(f)

with open(CONFIG['test_cases_file'], 'r', encoding='utf-8') as f:
    test_cases = json.load(f)

print(f"   âœ… {len(test_scenarios)} scÃ©narios chargÃ©s")
print(f"   âœ… {sum(len(cases) for cases in test_cases.values())} cas de test chargÃ©s\n")

# ============================================================================
# PHASE 3: EXÃ‰CUTION DES TESTS ET DÃ‰TECTION DE BUGS
# ============================================================================
print("ğŸ§ª [3/6] ExÃ©cution des tests et dÃ©tection de bugs...\n")

bugs_found = []
issues_found = []
warnings_found = []
test_results = {}

def predict_tweet(text):
    """PrÃ©diction pour un tweet donnÃ©"""
    try:
        if not text or text.strip() == '':
            return {
                'sentiment': 'neutre',
                'categorie': 'autre',
                'priority': 'basse',
                'error': None
            }
        
        X = vectorizer.transform([text])
        
        return {
            'sentiment': model_sentiment.predict(X)[0],
            'categorie': model_categorie.predict(X)[0],
            'priority': model_priority.predict(X)[0],
            'error': None
        }
    except Exception as e:
        return {
            'sentiment': None,
            'categorie': None,
            'priority': None,
            'error': str(e)
        }

# TEST 1: Edge Cases - Texte Vide
print("   [TEST 1/6] Edge Cases - Texte Vide...")
for case in test_cases['edge_case_empty_texts']:
    result = predict_tweet(case['text'])
    
    if result['error']:
        bugs_found.append({
            'id': f"BUG-{len(bugs_found)+1:03d}",
            'severity': 'HIGH',
            'category': 'edge_case',
            'test_case': case['id'],
            'description': f"Erreur avec texte vide: {result['error']}",
            'input': case['text'],
            'expected': 'default_prediction',
            'actual': 'error',
            'stack_trace': result['error']
        })
    elif result['sentiment'] is None:
        issues_found.append({
            'id': f"ISSUE-{len(issues_found)+1:03d}",
            'severity': 'MEDIUM',
            'category': 'edge_case',
            'test_case': case['id'],
            'description': f"PrÃ©diction None pour texte vide",
            'input': case['text']
        })

print(f"      âœ… TestÃ©: {len(test_cases['edge_case_empty_texts'])} cas")
print(f"      ğŸ› Bugs trouvÃ©s: {len([b for b in bugs_found if 'empty' in b.get('test_case', '').lower()])}")

# TEST 2: Edge Cases - CaractÃ¨res SpÃ©ciaux
print("   [TEST 2/6] Edge Cases - CaractÃ¨res SpÃ©ciaux...")
for case in test_cases['edge_case_special_characters']:
    result = predict_tweet(case['text'])
    
    if result['error']:
        bugs_found.append({
            'id': f"BUG-{len(bugs_found)+1:03d}",
            'severity': 'HIGH',
            'category': 'special_characters',
            'test_case': case['id'],
            'description': f"Erreur avec caractÃ¨res spÃ©ciaux: {result['error']}",
            'input': case['text']
        })

print(f"      âœ… TestÃ©: {len(test_cases['edge_case_special_characters'])} cas")

# TEST 3: Boundary Cases - AmbiguÃ¯tÃ©
print("   [TEST 3/6] Boundary Cases - AmbiguÃ¯tÃ©...")
ambiguity_issues = 0
for case in test_cases['boundary_case_ambiguous']:
    result = predict_tweet(case['text'])
    
    # VÃ©rifier si les prÃ©dictions sont cohÃ©rentes pour du texte ambigu
    if result['sentiment'] == 'positif' and 'problÃ¨me' in case['text'].lower():
        warnings_found.append({
            'id': f"WARN-{len(warnings_found)+1:03d}",
            'severity': 'LOW',
            'category': 'ambiguity',
            'test_case': case['id'],
            'description': f"Sentiment positif dÃ©tectÃ© avec mot nÃ©gatif",
            'input': case['text'],
            'prediction': result
        })
        ambiguity_issues += 1

print(f"      âœ… TestÃ©: {len(test_cases['boundary_case_ambiguous'])} cas")
print(f"      âš ï¸  Warnings: {ambiguity_issues}")

# TEST 4: Critical Cases - Urgence
print("   [TEST 4/6] Critical Cases - Urgence...")
urgent_misses = 0
for case in test_cases['critical_case_urgent']:
    result = predict_tweet(case['text'])
    
    if result['priority'] != 'haute' and case.get('expected_priority') == 'haute':
        issues_found.append({
            'id': f"ISSUE-{len(issues_found)+1:03d}",
            'severity': 'CRITICAL',
            'category': 'urgent_detection',
            'test_case': case['id'],
            'description': f"Tweet urgent non dÃ©tectÃ© comme prioritÃ© haute",
            'input': case['text'],
            'expected': 'haute',
            'actual': result['priority']
        })
        urgent_misses += 1

print(f"      âœ… TestÃ©: {len(test_cases['critical_case_urgent'])} cas")
print(f"      âŒ Misses: {urgent_misses}")

# TEST 5: Performance - Vitesse
print("   [TEST 5/6] Performance - Vitesse...")
start_time = time.time()
test_texts = ["test"] * 100
for text in test_texts:
    _ = predict_tweet(text)
elapsed = time.time() - start_time
avg_time_ms = (elapsed / len(test_texts)) * 1000

print(f"      âœ… 100 prÃ©dictions en {elapsed:.2f}s")
print(f"      â±ï¸  Temps moyen: {avg_time_ms:.2f}ms par tweet")

if avg_time_ms > 100:
    warnings_found.append({
        'id': f"WARN-{len(warnings_found)+1:03d}",
        'severity': 'MEDIUM',
        'category': 'performance',
        'description': f"Temps d'infÃ©rence Ã©levÃ©: {avg_time_ms:.2f}ms (seuil: 100ms)",
        'threshold': 100,
        'actual': avg_time_ms
    })

# TEST 6: Consistency
print("   [TEST 6/6] Consistency - RÃ©pÃ©tabilitÃ©...")
test_text = "problÃ¨me de connexion internet"
predictions = []
for _ in range(10):
    result = predict_tweet(test_text)
    predictions.append((result['sentiment'], result['categorie'], result['priority']))

unique_predictions = len(set(predictions))
print(f"      âœ… 10 prÃ©dictions effectuÃ©es")
print(f"      ğŸ”„ PrÃ©dictions uniques: {unique_predictions}")

if unique_predictions > 1:
    bugs_found.append({
        'id': f"BUG-{len(bugs_found)+1:03d}",
        'severity': 'HIGH',
        'category': 'consistency',
        'description': f"PrÃ©dictions inconsistantes pour le mÃªme input ({unique_predictions} variations)",
        'input': test_text,
        'predictions': [str(p) for p in set(predictions)]
    })

print()

# ============================================================================
# PHASE 4: ANALYSE DES RÃ‰SULTATS
# ============================================================================
print("ğŸ“Š [4/6] Analyse des rÃ©sultats...\n")

print(f"   ğŸ› BUGS CRITIQUES: {len([b for b in bugs_found if b['severity'] == 'HIGH'])}")
print(f"   âš ï¸  ISSUES: {len([i for i in issues_found if i['severity'] in ['CRITICAL', 'HIGH']])}")
print(f"   ğŸ’¡ WARNINGS: {len(warnings_found)}")
print(f"\n   Total de problÃ¨mes identifiÃ©s: {len(bugs_found) + len(issues_found) + len(warnings_found)}\n")

# ============================================================================
# PHASE 5: SAUVEGARDE DES RÃ‰SULTATS
# ============================================================================
print("ğŸ’¾ [5/6] Sauvegarde des rÃ©sultats...")

# Rapport de bug bash
bug_bash_report = {
    'metadata': {
        'date': datetime.now().isoformat(),
        'duration_minutes': 'ExÃ©cution automatisÃ©e',
        'tester': 'Automated Bug Bash Script',
        'models_tested': [
            'vectorizer',
            'sentiment_classifier',
            'categorie_classifier',
            'priority_classifier'
        ]
    },
    'summary': {
        'total_tests_run': 6,
        'bugs_found': len(bugs_found),
        'issues_found': len(issues_found),
        'warnings_found': len(warnings_found),
        'critical_issues': len([i for i in issues_found if i['severity'] == 'CRITICAL']),
        'high_priority_bugs': len([b for b in bugs_found if b['severity'] == 'HIGH'])
    },
    'bugs': bugs_found,
    'issues': issues_found,
    'warnings': warnings_found,
    'performance_metrics': {
        'average_inference_time_ms': round(avg_time_ms, 2),
        'throughput_tweets_per_second': round(1000 / avg_time_ms, 2) if avg_time_ms > 0 else 0
    }
}

report_file = os.path.join(CONFIG['output_dir'], f'bug_bash_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
with open(report_file, 'w', encoding='utf-8') as f:
    json.dump(bug_bash_report, f, indent=2, ensure_ascii=False)

print(f"   âœ… Rapport sauvegardÃ©: {report_file}\n")

# ============================================================================
# PHASE 6: GÃ‰NÃ‰RATION DU RAPPORT LISIBLE
# ============================================================================
print("ğŸ“„ [6/6] GÃ©nÃ©ration du rapport lisible...\n")

# Rapport markdown
markdown_report = f"""# ğŸ› Bug Bash Report - FreeMobilaChat
## Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š RÃ©sumÃ© ExÃ©cutif

| MÃ©trique | Valeur |
|----------|--------|
| **Tests exÃ©cutÃ©s** | {bug_bash_report['summary']['total_tests_run']} |
| **Bugs trouvÃ©s** | {bug_bash_report['summary']['bugs_found']} |
| **Issues identifiÃ©es** | {bug_bash_report['summary']['issues_found']} |
| **Warnings** | {bug_bash_report['summary']['warnings_found']} |
| **Issues critiques** | {bug_bash_report['summary']['critical_issues']} |
| **Bugs haute prioritÃ©** | {bug_bash_report['summary']['high_priority_bugs']} |

**Statut Global**: {"ğŸ”´ CRITIQUE" if bug_bash_report['summary']['critical_issues'] > 0 else "ğŸŸ  Ã€ SURVEILLER" if bug_bash_report['summary']['bugs_found'] > 0 else "ğŸŸ¢ BON"}

---

## ğŸ› Bugs IdentifiÃ©s ({len(bugs_found)})

"""

for bug in bugs_found:
    markdown_report += f"""
### {bug['id']} - {bug['severity']}
- **CatÃ©gorie**: {bug['category']}
- **Description**: {bug['description']}
- **Test Case**: {bug.get('test_case', 'N/A')}
- **Input**: `{bug.get('input', 'N/A')}`
- **Expected**: {bug.get('expected', 'N/A')}
- **Actual**: {bug.get('actual', 'N/A')}

"""

markdown_report += f"""
---

## âš ï¸  Issues IdentifiÃ©es ({len(issues_found)})

"""

for issue in issues_found:
    markdown_report += f"""
### {issue['id']} - {issue['severity']}
- **CatÃ©gorie**: {issue['category']}
- **Description**: {issue['description']}
- **Input**: `{issue.get('input', 'N/A')}`

"""

markdown_report += f"""
---

## ğŸ’¡ Warnings ({len(warnings_found)})

"""

for warn in warnings_found:
    markdown_report += f"""
### {warn['id']} - {warn['severity']}
- **CatÃ©gorie**: {warn['category']}
- **Description**: {warn['description']}

"""

markdown_report += f"""
---

## ğŸ¯ Performance

- **Temps moyen d'infÃ©rence**: {avg_time_ms:.2f}ms
- **Throughput**: {1000/avg_time_ms:.2f} tweets/seconde
- **Seuil acceptable**: 100ms
- **Statut**: {"âœ… BON" if avg_time_ms < 100 else "âš ï¸  AMÃ‰LIORATION NÃ‰CESSAIRE"}

---

## ğŸ“ Recommandations

"""

if bug_bash_report['summary']['critical_issues'] > 0:
    markdown_report += "1. **URGENT**: Corriger les {0} issues critiques avant le dÃ©ploiement\n".format(bug_bash_report['summary']['critical_issues'])

if bug_bash_report['summary']['high_priority_bugs'] > 0:
    markdown_report += "2. **HIGH**: Corriger les {0} bugs haute prioritÃ©\n".format(bug_bash_report['summary']['high_priority_bugs'])

if avg_time_ms > 100:
    markdown_report += "3. **PERFORMANCE**: Optimiser le temps d'infÃ©rence (actuel: {0:.2f}ms, cible: <100ms)\n".format(avg_time_ms)

if len(warnings_found) > 0:
    markdown_report += "4. **QUALITY**: Investiguer les {0} warnings pour amÃ©liorer la qualitÃ©\n".format(len(warnings_found))

markdown_report += """
---

*Rapport gÃ©nÃ©rÃ© automatiquement par le script de Bug Bash*
"""

markdown_file = os.path.join(CONFIG['output_dir'], f'bug_bash_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md')
with open(markdown_file, 'w', encoding='utf-8') as f:
    f.write(markdown_report)

print(f"   âœ… Rapport markdown sauvegardÃ©: {markdown_file}\n")

# ============================================================================
# RÃ‰SUMÃ‰ FINAL
# ============================================================================
print("â•”" + "="*78 + "â•—")
print("â•‘" + " "*25 + "âœ… BUG BASH COMPLÃ‰TÃ‰!" + " "*27 + "â•‘")
print("â•š" + "="*78 + "â•\n")

print("ğŸ“Š RÃ‰SULTATS FINAUX:\n")
print(f"   Tests ExÃ©cutÃ©s:        {bug_bash_report['summary']['total_tests_run']}")
print(f"   Bugs TrouvÃ©s:          {bug_bash_report['summary']['bugs_found']}")
print(f"   Issues IdentifiÃ©es:    {bug_bash_report['summary']['issues_found']}")
print(f"   Warnings:              {bug_bash_report['summary']['warnings_found']}")
print(f"\n   Issues Critiques:      {bug_bash_report['summary']['critical_issues']}")
print(f"   Bugs Haute PrioritÃ©:   {bug_bash_report['summary']['high_priority_bugs']}")

print(f"\nğŸ“ RAPPORTS GÃ‰NÃ‰RÃ‰S:")
print(f"   â€¢ {report_file}")
print(f"   â€¢ {markdown_file}")

print(f"\nğŸ¯ STATUT:")
if bug_bash_report['summary']['critical_issues'] > 0:
    print("   ğŸ”´ CRITIQUE - Corrections urgentes requises")
elif bug_bash_report['summary']['high_priority_bugs'] > 0:
    print("   ğŸŸ  ATTENTION - Bugs importants Ã  corriger")
else:
    print("   ğŸŸ¢ BON - ModÃ¨le fonctionnel avec warnings mineurs")

print("\n" + "="*80)
print("  ğŸ‰ Ã‰TAPE 4 COMPLÃ‰TÃ‰E AVEC SUCCÃˆS!")
print("="*80 + "\n")

print("ğŸ“– PROCHAINE Ã‰TAPE:")
print("   â†’ Ã‰tape 5: Fine-tuner BERT pour amÃ©liorer la prÃ©cision\n")

