"""
Optimized Lazy Loader - FreeMobilaChat
=======================================

SystÃ¨me de chargement optimisÃ© avec cache intelligent pour Ã©viter
les temps de chargement lents lors de l'upload de fichiers.

Features:
- Lazy loading des modules lourds (BERT, Transformers)
- Cache intelligent avec st.cache_resource
- Chargement asynchrone des modÃ¨les
- Gestion robuste des erreurs
"""

import streamlit as st
import logging
from typing import Optional, Callable, Any
from functools import wraps
import time

logger = logging.getLogger(__name__)


class OptimizedLoader:
    """
    Gestionnaire de chargement optimisÃ© pour les modÃ¨les ML lourds
    
    Utilise:
    - Lazy loading pour ne charger que quand nÃ©cessaire
    - Cache Streamlit pour rÃ©utiliser les modÃ¨les
    - Gestion d'erreurs robuste
    """
    
    _instances = {}  # Cache manuel en cas d'Ã©chec de st.cache_resource
    
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_bert_classifier(use_gpu: bool = False):
        """
        Charge le classificateur BERT avec cache intelligent
        
        Args:
            use_gpu: Utiliser GPU si disponible
            
        Returns:
            Instance de BERTClassifier ou None en cas d'erreur
        """
        try:
            logger.info("ğŸ”„ Chargement BERT Classifier (cached)...")
            start = time.time()
            
            # Import dynamique (lazy loading)
            from services.bert_classifier import BERTClassifier
            
            classifier = BERTClassifier(use_gpu=use_gpu)
            
            elapsed = time.time() - start
            logger.info(f"âœ… BERT chargÃ© en {elapsed:.2f}s (cache actif)")
            
            return classifier
            
        except Exception as e:
            logger.error(f"âŒ Erreur chargement BERT: {e}")
            return None
    
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_mistral_classifier(model_name: str = 'mistral'):
        """
        Charge le classificateur Mistral avec cache intelligent
        
        Args:
            model_name: Nom du modÃ¨le Ollama
            
        Returns:
            Instance de MistralClassifier ou None en cas d'erreur
        """
        try:
            logger.info("ğŸ”„ Chargement Mistral Classifier (cached)...")
            start = time.time()
            
            # Import dynamique (lazy loading)
            from services.mistral_classifier import MistralClassifier, check_ollama_availability
            
            # VÃ©rifier Ollama sans bloquer
            if not check_ollama_availability():
                logger.warning("âš ï¸ Ollama non disponible - Mode dÃ©gradÃ©")
                return None
            
            classifier = MistralClassifier(model_name=model_name)
            
            elapsed = time.time() - start
            logger.info(f"âœ… Mistral chargÃ© en {elapsed:.2f}s (cache actif)")
            
            return classifier
            
        except Exception as e:
            logger.error(f"âŒ Erreur chargement Mistral: {e}")
            return None
    
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_rule_classifier():
        """
        Charge le classificateur par rÃ¨gles (lÃ©ger, toujours disponible)
        
        Returns:
            Instance de EnhancedRuleClassifier
        """
        try:
            logger.info("ğŸ”„ Chargement Rule Classifier (cached)...")
            
            # Import dynamique
            from services.rule_classifier import EnhancedRuleClassifier
            
            classifier = EnhancedRuleClassifier()
            
            logger.info("âœ… Rule Classifier chargÃ©")
            
            return classifier
            
        except Exception as e:
            logger.error(f"âŒ Erreur chargement Rule Classifier: {e}")
            return None
    
    @staticmethod
    @st.cache_resource(show_spinner=False)
    def load_tweet_cleaner():
        """
        Charge le nettoyeur de tweets (lÃ©ger)
        
        Returns:
            Instance de TweetCleaner
        """
        try:
            from services.tweet_cleaner import TweetCleaner
            return TweetCleaner()
        except Exception as e:
            logger.error(f"âŒ Erreur chargement TweetCleaner: {e}")
            return None
    
    @staticmethod
    def check_ollama_availability() -> bool:
        """
        VÃ©rifie rapidement la disponibilitÃ© d'Ollama sans bloquer
        
        Returns:
            True si Ollama est disponible
        """
        try:
            # Import lazy
            import ollama
            
            # Test rapide (timeout 2s)
            ollama.list()
            return True
            
        except Exception as e:
            logger.warning(f"Ollama non disponible: {e}")
            return False
    
    @staticmethod
    def get_available_models():
        """
        Retourne les modÃ¨les de classification disponibles
        
        Returns:
            dict: {'bert': bool, 'mistral': bool, 'rules': bool}
        """
        available = {
            'bert': False,
            'mistral': False,
            'rules': True  # Toujours disponible
        }
        
        # Test BERT (rapide)
        try:
            import torch
            from transformers import AutoTokenizer
            available['bert'] = True
        except:
            pass
        
        # Test Mistral
        available['mistral'] = OptimizedLoader.check_ollama_availability()
        
        return available


def lazy_import(module_name: str, item_name: str = None):
    """
    Importe un module ou une classe de faÃ§on lazy
    
    Args:
        module_name: Nom du module (ex: 'services.bert_classifier')
        item_name: Nom de la classe/fonction Ã  importer (optionnel)
        
    Returns:
        Module ou classe importÃ©e
        
    Example:
        BERTClassifier = lazy_import('services.bert_classifier', 'BERTClassifier')
    """
    import importlib
    
    module = importlib.import_module(module_name)
    
    if item_name:
        return getattr(module, item_name)
    
    return module


def with_spinner(message: str = "Chargement..."):
    """
    DÃ©corateur pour ajouter un spinner Streamlit aux fonctions lourdes
    
    Args:
        message: Message Ã  afficher pendant le chargement
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            with st.spinner(message):
                return func(*args, **kwargs)
        return wrapper
    return decorator


# Singleton pour Ã©viter rechargements multiples
_loader_instance = None

def get_loader() -> OptimizedLoader:
    """
    Retourne l'instance unique du loader (pattern Singleton)
    
    Returns:
        OptimizedLoader instance
    """
    global _loader_instance
    
    if _loader_instance is None:
        _loader_instance = OptimizedLoader()
    
    return _loader_instance


if __name__ == "__main__":
    # Tests
    loader = get_loader()
    
    print("ğŸ§ª Test du loader optimisÃ©...")
    
    # Test des modÃ¨les disponibles
    available = loader.get_available_models()
    print(f"ModÃ¨les disponibles: {available}")
    
    # Test chargement rules (lÃ©ger)
    rules = loader.load_rule_classifier()
    print(f"Rules classifier: {'âœ…' if rules else 'âŒ'}")
    
    print("âœ… Tests terminÃ©s")

