"""
Processeur ParallÃ¨le OptimisÃ© - FreeMobilaChat
===============================================

ParallÃ©lisation intelligente pour classification rapide.
OptimisÃ© pour i9-13900H (20 threads).

Performance: 5000 tweets en < 2 minutes
"""

from typing import List, Dict, Callable, Any
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from tqdm import tqdm
import time

logger = logging.getLogger(__name__)


class ParallelProcessor:
    """
    Processeur parallÃ¨le pour classification multi-thread
    
    Utilise ThreadPoolExecutor pour parallÃ©liser les appels Ã  Mistral.
    OptimisÃ© pour machines multi-core (i9-13900H = 20 threads).
    """
    
    def __init__(self, n_workers: int = 8, show_progress: bool = True):
        """
        Initialise le processeur parallÃ¨le
        
        Args:
            n_workers: Nombre de workers (8 recommandÃ© pour i9-13900H)
            show_progress: Afficher progress bar
        """
        self.n_workers = n_workers
        self.show_progress = show_progress
        
        logger.info(f"ðŸ”§ Processeur parallÃ¨le: {n_workers} workers")
    
    def process_dataframe_parallel(self,
                                   df: pd.DataFrame,
                                   process_func: Callable,
                                   chunk_size: int = 50) -> pd.DataFrame:
        """
        Traite un DataFrame en parallÃ¨le
        
        Args:
            df: DataFrame Ã  traiter
            process_func: Fonction Ã  appliquer (ex: classifier.classify_batch)
            chunk_size: Taille des chunks
            
        Returns:
            DataFrame avec rÃ©sultats combinÃ©s
        """
        # DÃ©couper en chunks
        chunks = [
            df.iloc[i:i+chunk_size].copy()
            for i in range(0, len(df), chunk_size)
        ]
        
        logger.info(f"ðŸ“¦ {len(chunks)} chunks de {chunk_size} tweets")
        
        # Traitement parallÃ¨le
        results = []
        
        with ThreadPoolExecutor(max_workers=self.n_workers) as executor:
            # Soumettre tous les jobs
            futures = {
                executor.submit(process_func, chunk): idx
                for idx, chunk in enumerate(chunks)
            }
            
            # Collecter rÃ©sultats avec progress
            if self.show_progress:
                pbar = tqdm(total=len(futures), desc="âš¡ Traitement parallÃ¨le")
            
            for future in as_completed(futures):
                idx = futures[future]
                try:
                    result = future.result()
                    results.append((idx, result))
                    
                    if self.show_progress:
                        pbar.update(1)
                        
                except Exception as e:
                    logger.error(f"Erreur chunk {idx}: {e}")
                    # Fallback: retourner le chunk original
                    results.append((idx, chunks[idx]))
            
            if self.show_progress:
                pbar.close()
        
        # Recombiner dans l'ordre
        results.sort(key=lambda x: x[0])
        combined = pd.concat([r[1] for r in results], ignore_index=True)
        
        return combined
    
    def process_list_parallel(self,
                             items: List[any],
                             process_func: Callable,
                             batch_size: int = 25) -> List[any]:
        """
        Traite une liste en parallÃ¨le
        
        Args:
            items: Liste Ã  traiter
            process_func: Fonction Ã  appliquer
            batch_size: Taille des batches
            
        Returns:
            Liste des rÃ©sultats
        """
        # DÃ©couper en batches
        batches = [
            items[i:i+batch_size]
            for i in range(0, len(items), batch_size)
        ]
        
        results = []
        
        with ThreadPoolExecutor(max_workers=self.n_workers) as executor:
            futures = [executor.submit(process_func, batch) for batch in batches]
            
            if self.show_progress:
                futures_iter = tqdm(
                    as_completed(futures),
                    total=len(futures),
                    desc="âš¡ Traitement"
                )
            else:
                futures_iter = as_completed(futures)
            
            for future in futures_iter:
                try:
                    result = future.result()
                    results.extend(result)
                except Exception as e:
                    logger.error(f"Erreur traitement: {e}")
        
        return results


class SmartBatchOptimizer:
    """
    Optimiseur de taille de batch selon les ressources
    """
    
    @staticmethod
    def calculate_optimal_batch_size(
        n_tweets: int,
        available_memory_gb: float,
        use_gpu: bool = False
    ) -> int:
        """
        Calcule la taille optimale de batch
        
        Args:
            n_tweets: Nombre total de tweets
            available_memory_gb: MÃ©moire disponible
            use_gpu: GPU disponible
            
        Returns:
            Taille de batch optimale
        """
        if use_gpu:
            # GPU: gros batches
            if available_memory_gb >= 8:  # RTX 5060 = 8GB
                return 64
            elif available_memory_gb >= 4:
                return 32
            else:
                return 16
        else:
            # CPU: batches moyens
            if available_memory_gb >= 16:
                return 32
            elif available_memory_gb >= 8:
                return 16
            else:
                return 8
    
    @staticmethod
    def calculate_optimal_workers(cpu_count: int, task_type: str = 'io_bound') -> int:
        """
        Calcule le nombre optimal de workers
        
        Args:
            cpu_count: Nombre de cores CPU
            task_type: 'io_bound' ou 'cpu_bound'
            
        Returns:
            Nombre de workers
        """
        if task_type == 'io_bound':
            # I/O bound (Ollama API): 2x cores
            return min(cpu_count * 2, 16)
        else:
            # CPU bound (calculs): cores - 2
            return max(cpu_count - 2, 4)


class ProgressTracker:
    """Tracker de progression pour UI Streamlit"""
    
    def __init__(self, total: int, description: str = "Traitement"):
        self.total = total
        self.description = description
        self.current = 0
        self.start_time = time.time()
    
    def update(self, n: int = 1):
        """Met Ã  jour la progression"""
        self.current += n
        
        elapsed = time.time() - self.start_time
        rate = self.current / elapsed if elapsed > 0 else 0
        remaining = (self.total - self.current) / rate if rate > 0 else 0
        
        return {
            'current': self.current,
            'total': self.total,
            'percentage': (self.current / self.total * 100) if self.total > 0 else 0,
            'elapsed': elapsed,
            'rate': rate,
            'remaining': remaining
        }


if __name__ == '__main__':
    # Test du processeur parallÃ¨le
    print("\nðŸ§ª Test du processeur parallÃ¨le\n")
    
    import os
    
    # DÃ©tection ressources
    cpu_count = os.cpu_count()
    print(f"ðŸ’» CPU cores: {cpu_count}")
    
    # Test fonction simple
    def test_func(chunk):
        """Fonction test qui simule un traitement"""
        time.sleep(0.1)  # Simuler traitement
        return [{'result': item} for item in chunk]
    
    # Test data
    test_data = list(range(100))
    
    # Traitement parallÃ¨le
    processor = ParallelProcessor(n_workers=4)
    start = time.time()
    results = processor.process_list_parallel(test_data, test_func, batch_size=25)
    elapsed = time.time() - start
    
    print(f"âœ… {len(results)} items traitÃ©s en {elapsed:.2f}s")
    print(f"âš¡ Vitesse: {len(results)/elapsed:.1f} items/s")
    
    # Test optimiseur
    optimizer = SmartBatchOptimizer()
    batch_size = optimizer.calculate_optimal_batch_size(5000, 32, use_gpu=True)
    workers = optimizer.calculate_optimal_workers(cpu_count, 'io_bound')
    
    print(f"\nðŸ“Š Configuration optimale pour votre machine:")
    print(f"   Batch size GPU: {batch_size}")
    print(f"   Workers: {workers}")
    
    print("\nâœ… Test terminÃ©!")

