"""
Documentation Scraper for FreeMobilaChat SAV Knowledge Base
Scrapes Free Mobile assistance documentation and creates embeddings for semantic search
"""

import asyncio
import hashlib
import logging
import re
from datetime import datetime, UTC
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
from urllib.parse import urljoin, urlparse
import json

# Web scraping imports
try:
    import requests
    from bs4 import BeautifulSoup
    import aiohttp
    SCRAPING_AVAILABLE = True
except ImportError:
    SCRAPING_AVAILABLE = False
    print("Web scraping dependencies not available. Install: pip install requests beautifulsoup4 aiohttp")

# Embeddings imports
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    print("Embeddings dependencies not available. Install: pip install sentence-transformers")

from ..models import KnowledgeDocument, DocumentType
from ..utils.database import DatabaseManager

logger = logging.getLogger(__name__)

class DocumentationScraper:
    """Service pour scraper et indexer la documentation Free Mobile"""
    
    # URLs de base de la documentation Free Mobile
    BASE_URLS = [
        "https://assistance.free.fr/",
        "https://assistance.free.fr/univers/mobile-free/",
        "https://assistance.free.fr/univers/mobile-free/mon-mobile/premiers-pas",
        "https://assistance.free.fr/articles/achat-de-telephone-garanties-legales-et-service-apres-vente-965"
    ]
    
    def __init__(self, db_manager: Optional[DatabaseManager] = None):
        """
        Initialiser le scraper de documentation
        
        Args:
            db_manager: Gestionnaire de base de donn√©es
        """
        self.db_manager = db_manager
        self.logger = logging.getLogger(__name__)
        
        # V√©rifier les d√©pendances (mode d√©grad√© si non disponibles)
        if not SCRAPING_AVAILABLE:
            self.logger.warning(" Web scraping dependencies not available. Scraping will be disabled.")

        if not EMBEDDINGS_AVAILABLE:
            self.logger.warning(" Embeddings dependencies not available. Semantic search will be disabled.")
        
        # Initialiser le mod√®le d'embeddings (mod√®le fran√ßais optimis√©)
        self.embedding_model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        self.embedding_model = None
        self._initialize_embedding_model()
        
        # Configuration du scraping
        self.session_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # Cache pour √©viter de scraper les m√™mes pages
        self.scraped_urls = set()
        self.failed_urls = set()
    
    def _initialize_embedding_model(self):
        """Initialiser le mod√®le d'embeddings"""
        if not EMBEDDINGS_AVAILABLE:
            self.logger.warning(" Mod√®le d'embeddings non disponible (d√©pendances manquantes)")
            self.embedding_model = None
            return

        try:
            self.logger.info(f"ü§ñ Initialisation du mod√®le d'embeddings: {self.embedding_model_name}")
            self.embedding_model = SentenceTransformer(self.embedding_model_name)
            self.logger.info(" Mod√®le d'embeddings initialis√© avec succ√®s")
        except Exception as e:
            self.logger.error(f" Erreur lors de l'initialisation du mod√®le d'embeddings: {e}")
            self.embedding_model = None
    
    def _clean_text(self, text: str) -> str:
        """
        Nettoyer et normaliser le texte extrait
        
        Args:
            text: Texte brut √† nettoyer
            
        Returns:
            Texte nettoy√©
        """
        if not text:
            return ""
        
        # Supprimer les espaces multiples et les retours √† la ligne
        text = re.sub(r'\s+', ' ', text)
        
        # Supprimer les caract√®res de contr√¥le
        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
        
        # Supprimer les espaces en d√©but et fin
        text = text.strip()
        
        return text
    
    def _extract_content_from_html(self, html: str, url: str) -> Tuple[str, str, DocumentType]:
        """
        Extraire le contenu principal d'une page HTML
        
        Args:
            html: Code HTML de la page
            url: URL de la page
            
        Returns:
            Tuple (titre, contenu, type_document)
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Supprimer les scripts, styles et autres √©l√©ments non pertinents
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
            
            # Extraire le titre
            title = ""
            title_tag = soup.find('title')
            if title_tag:
                title = self._clean_text(title_tag.get_text())
            
            # Si pas de titre dans <title>, chercher dans h1
            if not title:
                h1_tag = soup.find('h1')
                if h1_tag:
                    title = self._clean_text(h1_tag.get_text())
            
            # Extraire le contenu principal
            content_selectors = [
                'main',
                '.content',
                '.main-content',
                '.article-content',
                '.page-content',
                'article',
                '.container'
            ]
            
            content = ""
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    content = self._clean_text(content_element.get_text())
                    break
            
            # Si aucun s√©lecteur sp√©cifique trouv√©, prendre le body
            if not content:
                body = soup.find('body')
                if body:
                    content = self._clean_text(body.get_text())
            
            # D√©terminer le type de document bas√© sur l'URL et le contenu
            document_type = self._determine_document_type(url, title, content)
            
            return title, content, document_type
            
        except Exception as e:
            self.logger.error(f" Erreur lors de l'extraction du contenu de {url}: {e}")
            return "", "", DocumentType.GENERAL
    
    def _determine_document_type(self, url: str, title: str, content: str) -> DocumentType:
        """
        D√©terminer le type de document bas√© sur l'URL et le contenu
        
        Args:
            url: URL du document
            title: Titre du document
            content: Contenu du document
            
        Returns:
            Type de document
        """
        url_lower = url.lower()
        title_lower = title.lower()
        content_lower = content.lower()
        
        # Mots-cl√©s pour chaque type
        faq_keywords = ['faq', 'questions', 'r√©ponses', 'comment', 'pourquoi']
        guide_keywords = ['guide', 'tutoriel', '√©tapes', 'proc√©dure', 'premiers pas']
        troubleshooting_keywords = ['probl√®me', 'panne', 'erreur', 'd√©pannage', 'r√©soudre', 'solution']
        procedure_keywords = ['configuration', 'param√©trage', 'installation', 'activation']
        
        # V√©rifier dans l'URL d'abord
        if any(keyword in url_lower for keyword in faq_keywords):
            return DocumentType.FAQ
        if any(keyword in url_lower for keyword in guide_keywords):
            return DocumentType.GUIDE
        if any(keyword in url_lower for keyword in troubleshooting_keywords):
            return DocumentType.TROUBLESHOOTING
        if any(keyword in url_lower for keyword in procedure_keywords):
            return DocumentType.PROCEDURE
        
        # V√©rifier dans le titre
        if any(keyword in title_lower for keyword in faq_keywords):
            return DocumentType.FAQ
        if any(keyword in title_lower for keyword in guide_keywords):
            return DocumentType.GUIDE
        if any(keyword in title_lower for keyword in troubleshooting_keywords):
            return DocumentType.TROUBLESHOOTING
        if any(keyword in title_lower for keyword in procedure_keywords):
            return DocumentType.PROCEDURE
        
        # V√©rifier dans le contenu (√©chantillon)
        content_sample = content_lower[:1000]  # Premier 1000 caract√®res
        if any(keyword in content_sample for keyword in faq_keywords):
            return DocumentType.FAQ
        if any(keyword in content_sample for keyword in guide_keywords):
            return DocumentType.GUIDE
        if any(keyword in content_sample for keyword in troubleshooting_keywords):
            return DocumentType.TROUBLESHOOTING
        if any(keyword in content_sample for keyword in procedure_keywords):
            return DocumentType.PROCEDURE
        
        return DocumentType.GENERAL
    
    def _generate_content_hash(self, content: str) -> str:
        """
        G√©n√©rer un hash SHA-256 du contenu pour d√©tecter les changements
        
        Args:
            content: Contenu √† hasher
            
        Returns:
            Hash SHA-256 en hexad√©cimal
        """
        return hashlib.sha256(content.encode('utf-8')).hexdigest()
    
    def _generate_embeddings(self, text: str) -> Optional[List[float]]:
        """
        G√©n√©rer les embeddings pour un texte
        
        Args:
            text: Texte √† encoder
            
        Returns:
            Vecteur d'embeddings ou None si erreur
        """
        try:
            if not self.embedding_model:
                self.logger.error(" Mod√®le d'embeddings non initialis√©")
                return None
            
            # Limiter la taille du texte pour √©viter les erreurs de m√©moire
            max_length = 512  # Limite du mod√®le
            if len(text) > max_length:
                text = text[:max_length]
            
            embeddings = self.embedding_model.encode(text, convert_to_tensor=False)
            return embeddings.tolist()
            
        except Exception as e:
            self.logger.error(f" Erreur lors de la g√©n√©ration des embeddings: {e}")
            return None

    async def scrape_url(self, url: str) -> Optional[KnowledgeDocument]:
        """
        Scraper une URL et cr√©er un document de connaissances

        Args:
            url: URL √† scraper

        Returns:
            Document de connaissances ou None si erreur
        """
        if url in self.scraped_urls or url in self.failed_urls:
            self.logger.info(f"URL already processed: {url}")
            return None

        try:
            self.logger.info(f"Scraping URL: {url}")

            async with aiohttp.ClientSession(headers=self.session_headers) as session:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                    if response.status != 200:
                        self.logger.warning(f" Statut HTTP {response.status} pour {url}")
                        self.failed_urls.add(url)
                        return None

                    html = await response.text()

            # Extraire le contenu
            title, content, document_type = self._extract_content_from_html(html, url)

            if not content or len(content) < 50:
                self.logger.warning(f" Contenu insuffisant pour {url}")
                self.failed_urls.add(url)
                return None

            # G√©n√©rer le hash du contenu
            content_hash = self._generate_content_hash(content)

            # G√©n√©rer les embeddings
            embeddings = self._generate_embeddings(content)
            if not embeddings:
                self.logger.warning(f" Impossible de g√©n√©rer les embeddings pour {url}")

            # Extraire le domaine
            parsed_url = urlparse(url)
            source_domain = parsed_url.netloc

            # Cr√©er le document
            document = KnowledgeDocument(
                title=title or f"Document de {source_domain}",
                content=content,
                document_type=document_type,
                source_url=url,
                source_domain=source_domain,
                content_hash=content_hash,
                embedding_model=self.embedding_model_name if embeddings else None,
                embedding_dimension=len(embeddings) if embeddings else None
            )

            self.scraped_urls.add(url)
            self.logger.info(f" Document cr√©√©: {title[:50]}... ({len(content)} caract√®res)")

            return document

        except Exception as e:
            self.logger.error(f" Erreur lors du scraping de {url}: {e}")
            self.failed_urls.add(url)
            return None

    async def scrape_all_documentation(self) -> List[KnowledgeDocument]:
        """
        Scraper toute la documentation Free Mobile

        Returns:
            Liste des documents cr√©√©s
        """
        self.logger.info(" D√©but du scraping de la documentation Free Mobile")

        documents = []

        # Scraper les URLs de base
        for url in self.BASE_URLS:
            document = await self.scrape_url(url)
            if document:
                documents.append(document)

            # Pause entre les requ√™tes pour √™tre respectueux
            await asyncio.sleep(1)

        self.logger.info(f" Scraping termin√©: {len(documents)} documents cr√©√©s")
        self.logger.info(f" URLs √©chou√©es: {len(self.failed_urls)}")

        return documents

    async def store_documents(self, documents: List[KnowledgeDocument]) -> int:
        """
        Stocker les documents dans la base de donn√©es

        Args:
            documents: Liste des documents √† stocker

        Returns:
            Nombre de documents stock√©s
        """
        if not self.db_manager:
            self.logger.error(" Gestionnaire de base de donn√©es non disponible")
            return 0

        stored_count = 0

        for document in documents:
            try:
                # V√©rifier si le document existe d√©j√† (par hash de contenu)
                existing = await self._check_existing_document(document.content_hash)

                if existing:
                    self.logger.info(f"Document already exists: {document.title[:50]}...")
                    continue

                # Stocker le document
                await self._store_single_document(document)
                stored_count += 1
                self.logger.info(f"Document stored: {document.title[:50]}...")

            except Exception as e:
                self.logger.error(f" Erreur lors du stockage de {document.title[:50]}...: {e}")

        self.logger.info(f" Stockage termin√©: {stored_count} nouveaux documents")
        return stored_count

    async def _check_existing_document(self, content_hash: str) -> bool:
        """
        V√©rifier si un document existe d√©j√† par son hash

        Args:
            content_hash: Hash du contenu √† v√©rifier

        Returns:
            True si le document existe d√©j√†
        """
        try:
            # Cette m√©thode devra √™tre impl√©ment√©e selon le syst√®me de base de donn√©es
            # Pour l'instant, on retourne False (pas de v√©rification)
            return False
        except Exception as e:
            self.logger.error(f" Erreur lors de la v√©rification d'existence: {e}")
            return False

    async def _store_single_document(self, document: KnowledgeDocument):
        """
        Stocker un seul document dans la base de donn√©es

        Args:
            document: Document √† stocker
        """
        try:
            # Cette m√©thode devra √™tre impl√©ment√©e selon le syst√®me de base de donn√©es
            # Pour l'instant, on log juste les informations
            self.logger.info(f" Stockage du document: {document.title}")
            self.logger.debug(f"   - URL: {document.source_url}")
            self.logger.debug(f"   - Type: {document.document_type}")
            self.logger.debug(f"   - Taille: {len(document.content)} caract√®res")
            self.logger.debug(f"   - Hash: {document.content_hash}")

        except Exception as e:
            self.logger.error(f" Erreur lors du stockage: {e}")
            raise

    async def update_knowledge_base(self) -> Dict[str, Any]:
        """
        Mettre √† jour compl√®tement la base de connaissances

        Returns:
            R√©sum√© de l'op√©ration
        """
        start_time = datetime.now(UTC)
        self.logger.info("üîÑ D√©but de la mise √† jour de la base de connaissances")

        try:
            # Scraper tous les documents
            documents = await self.scrape_all_documentation()

            # Stocker les documents
            stored_count = await self.store_documents(documents)

            end_time = datetime.now(UTC)
            duration = (end_time - start_time).total_seconds()

            result = {
                'success': True,
                'start_time': start_time,
                'end_time': end_time,
                'duration_seconds': duration,
                'total_urls_processed': len(self.scraped_urls) + len(self.failed_urls),
                'successful_scrapes': len(documents),
                'failed_scrapes': len(self.failed_urls),
                'documents_stored': stored_count,
                'failed_urls': list(self.failed_urls)
            }

            self.logger.info(f" Mise √† jour termin√©e en {duration:.1f}s")
            self.logger.info(f" R√©sum√©: {len(documents)} documents scrap√©s, {stored_count} stock√©s")

            return result

        except Exception as e:
            self.logger.error(f" Erreur lors de la mise √† jour: {e}")
            return {
                'success': False,
                'error': str(e),
                'start_time': start_time,
                'end_time': datetime.now(UTC)
            }
