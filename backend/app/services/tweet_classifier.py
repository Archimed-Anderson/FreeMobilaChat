"""
Module de Classification Intelligente des Tweets Free
======================================================

Ce module impl√©mente un syst√®me de classification multi-label pour les tweets
adress√©s √† @Free, bas√© sur un mod√®le LLM fine-tun√©.

Architecture:
    - D√©tection de r√©clamation (OUI/NON)
    - Classification th√©matique (FIBRE, MOBILE, TV, etc.)
    - Analyse de sentiment (NEGATIF, NEUTRE, POSITIF)
    - Niveau d'urgence (FAIBLE, MOYENNE, ELEVEE, CRITIQUE)
    - Type d'incident (PANNE, LENTEUR, FACTURATION, etc.)

Auteur: Archimed Anderson
Date: Octobre 2024
"""

import json
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime
from pydantic import BaseModel, Field, validator
import pandas as pd
import numpy as np
from pathlib import Path

# Imports pour LLM
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# Logger
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


class ClassificationResult(BaseModel):
    """
    R√©sultat structur√© de la classification d'un tweet.
    
    Attributes:
        is_reclamation: Indique si le tweet est une r√©clamation (OUI/NON)
        theme: Th√©matique principale (FIBRE, MOBILE, TV, FACTURE, SAV, RESEAU, AUTRE)
        sentiment: Sentiment exprim√© (NEGATIF, NEUTRE, POSITIF)
        urgence: Niveau d'urgence (FAIBLE, MOYENNE, ELEVEE, CRITIQUE)
        type_incident: Type d'incident (PANNE, LENTEUR, FACTURATION, PROCESSUS_SAV, INFO, AUTRE)
        confidence: Score de confiance de la classification (0-1)
        justification: Explication textuelle de la classification
        tweet_id: Identifiant du tweet classifi√©
        timestamp: Horodatage de la classification
    """
    is_reclamation: str = Field(..., description="OUI ou NON")
    theme: str = Field(..., description="FIBRE, MOBILE, TV, FACTURE, SAV, RESEAU, AUTRE")
    sentiment: str = Field(..., description="NEGATIF, NEUTRE, POSITIF")
    urgence: str = Field(..., description="FAIBLE, MOYENNE, ELEVEE, CRITIQUE")
    type_incident: str = Field(..., description="PANNE, LENTEUR, FACTURATION, PROCESSUS_SAV, INFO, AUTRE")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Score de confiance")
    justification: str = Field(..., description="Explication de la classification")
    tweet_id: Optional[str] = Field(None, description="ID du tweet")
    timestamp: datetime = Field(default_factory=datetime.now)
    
    @validator('is_reclamation')
    def validate_reclamation(cls, v):
        """Valide que is_reclamation est OUI ou NON"""
        if v not in ["OUI", "NON"]:
            raise ValueError(f"is_reclamation doit √™tre 'OUI' ou 'NON', pas '{v}'")
        return v
    
    @validator('theme')
    def validate_theme(cls, v):
        """Valide le th√®me"""
        valid_themes = ["FIBRE", "MOBILE", "TV", "FACTURE", "SAV", "RESEAU", "AUTRE"]
        if v not in valid_themes:
            raise ValueError(f"theme doit √™tre parmi {valid_themes}, pas '{v}'")
        return v
    
    @validator('sentiment')
    def validate_sentiment(cls, v):
        """Valide le sentiment"""
        valid_sentiments = ["NEGATIF", "NEUTRE", "POSITIF"]
        if v not in valid_sentiments:
            raise ValueError(f"sentiment doit √™tre parmi {valid_sentiments}, pas '{v}'")
        return v
    
    @validator('urgence')
    def validate_urgence(cls, v):
        """Valide le niveau d'urgence"""
        valid_urgence = ["FAIBLE", "MOYENNE", "ELEVEE", "CRITIQUE"]
        if v not in valid_urgence:
            raise ValueError(f"urgence doit √™tre parmi {valid_urgence}, pas '{v}'")
        return v
    
    @validator('type_incident')
    def validate_type_incident(cls, v):
        """Valide le type d'incident"""
        valid_types = ["PANNE", "LENTEUR", "FACTURATION", "PROCESSUS_SAV", "INFO", "AUTRE"]
        if v not in valid_types:
            raise ValueError(f"type_incident doit √™tre parmi {valid_types}, pas '{v}'")
        return v


class TweetClassifier:
    """
    Classificateur de tweets bas√© sur LLM avec few-shot learning.
    
    Ce classificateur utilise un prompt engineering avanc√© avec des exemples
    few-shot pour guider le mod√®le LLM dans la classification multi-label
    des tweets Free.
    
    Attributes:
        model_name: Nom du mod√®le LLM √† utiliser
        api_key: Cl√© API pour le service LLM
        temperature: Temp√©rature pour la g√©n√©ration (0 = d√©terministe)
        max_tokens: Nombre maximum de tokens dans la r√©ponse
    """
    
    # Exemples few-shot pour ancrage logique
    FEW_SHOT_EXAMPLES = [
        {
            "tweet": "üì¢ Free annonce le d√©ploiement de la fibre dans 200 nouvelles communes ! #Free #Fibre",
            "classification": {
                "is_reclamation": "NON",
                "theme": "FIBRE",
                "sentiment": "POSITIF",
                "urgence": "FAIBLE",
                "type_incident": "INFO"
            },
            "justification": "Annonce officielle, aucun probl√®me exprim√©"
        },
        {
            "tweet": "@Free √áa fait 3h que la fibre est coup√©e √† Lyon 3, c'est pr√©vu pour quand la r√©paration ? #PanneFibre",
            "classification": {
                "is_reclamation": "OUI",
                "theme": "FIBRE",
                "sentiment": "NEGATIF",
                "urgence": "ELEVEE",
                "type_incident": "PANNE"
            },
            "justification": "Panne d√©clar√©e, dur√©e pr√©cis√©e, demande de r√©solution urgente"
        },
        {
            "tweet": "@Free J'attends depuis 2 semaines une r√©ponse du SAV pour ma box, toujours rien ! C'est une blague ?",
            "classification": {
                "is_reclamation": "OUI",
                "theme": "SAV",
                "sentiment": "NEGATIF",
                "urgence": "MOYENNE",
                "type_incident": "PROCESSUS_SAV"
            },
            "justification": "Insatisfaction envers le d√©lai de traitement du service client"
        },
        {
            "tweet": "@Free Mon mobile Free a un d√©bit 4G tr√®s lent depuis hier, c'est normal ?",
            "classification": {
                "is_reclamation": "OUI",
                "theme": "MOBILE",
                "sentiment": "NEGATIF",
                "urgence": "MOYENNE",
                "type_incident": "LENTEUR"
            },
            "justification": "Probl√®me de performance r√©seau mobile signal√©"
        },
        {
            "tweet": "@Free Ma facture de ce mois est anormalement √©lev√©e, pouvez-vous v√©rifier ?",
            "classification": {
                "is_reclamation": "OUI",
                "theme": "FACTURE",
                "sentiment": "NEGATIF",
                "urgence": "MOYENNE",
                "type_incident": "FACTURATION"
            },
            "justification": "Contestation de facturation, demande de v√©rification"
        }
    ]
    
    # Prompt syst√®me avec taxonomie d√©taill√©e
    SYSTEM_PROMPT = """Tu es un expert en analyse de tweets du service client Free.
Ta mission est de classifier chaque tweet selon une taxonomie multi-label pr√©cise.

# TAXONOMIE DE CLASSIFICATION

## 1. D√©tection R√©clamation (is_reclamation)
- OUI : Le tweet exprime un probl√®me, une insatisfaction ou demande une r√©solution
- NON : Tweet informatif, positif, ou simple question sans r√©clamation

## 2. Th√©matique Principale (theme)
- FIBRE : Connexion internet fibre optique
- MOBILE : Forfait mobile, r√©seau 4G/5G, t√©l√©phonie
- TV : Freebox TV, cha√Ænes, d√©codeur
- FACTURE : Facturation, prix, pr√©l√®vements
- SAV : Service apr√®s-vente, support client
- RESEAU : Infrastructure r√©seau, couverture
- AUTRE : Autre th√©matique

## 3. Analyse Sentiment (sentiment)
- NEGATIF : Insatisfaction, col√®re, frustration
- NEUTRE : Question factuelle, demande d'information
- POSITIF : Satisfaction, remerciement, compliment

## 4. Niveau d'Urgence (urgence)
- FAIBLE : Information, question non urgente
- MOYENNE : Probl√®me g√™nant mais pas bloquant
- ELEVEE : Probl√®me impactant significativement le service
- CRITIQUE : Service totalement indisponible, impact majeur

## 5. Type d'Incident (type_incident)
- PANNE : Service totalement indisponible
- LENTEUR : D√©gradation de performance
- FACTURATION : Probl√®me de facturation ou paiement
- PROCESSUS_SAV : Probl√®me avec le processus du service client
- INFO : Information ou annonce
- AUTRE : Autre type

# R√àGLES DE CLASSIFICATION

1. Un tweet peut √™tre multi-th√©matique, mais tu dois identifier le th√®me PRINCIPAL
2. Pour is_reclamation=OUI, il faut une demande explicite ou implicite de r√©solution
3. L'urgence doit refl√©ter l'impact r√©el du probl√®me, pas seulement le ton
4. La justification doit √™tre concise mais pr√©cise (1-2 phrases max)
5. Le score de confiance (confidence) doit refl√©ter ta certitude :
   - 0.9-1.0 : Classification √©vidente
   - 0.7-0.9 : Classification probable
   - 0.5-0.7 : Classification incertaine (cas ambigu)
   - < 0.5 : Tr√®s incertain (√† reviewer manuellement)

# FORMAT DE R√âPONSE

Tu dois TOUJOURS r√©pondre en JSON valide strict avec cette structure :
{
    "is_reclamation": "OUI" ou "NON",
    "theme": "...",
    "sentiment": "...",
    "urgence": "...",
    "type_incident": "...",
    "confidence": 0.XX,
    "justification": "..."
}

AUCUN texte en dehors du JSON. Seulement le JSON brut."""
    
    def __init__(
        self,
        model_name: str = "gpt-4",
        api_key: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: int = 300
    ):
        """
        Initialise le classificateur.
        
        Args:
            model_name: Nom du mod√®le (gpt-4, gpt-3.5-turbo, claude-3, etc.)
            api_key: Cl√© API pour le service LLM
            temperature: Temp√©rature (0 = d√©terministe, 1 = cr√©atif)
            max_tokens: Tokens max dans la r√©ponse
        """
        self.model_name = model_name
        self.api_key = api_key
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # Initialiser le client LLM
        if model_name.lower() == "fallback":
            logger.warning("Mode fallback activ√©, utilisation des r√®gles de classification")
            self.client = None
            self.provider = "fallback"
        elif "gpt" in model_name.lower():
            if not OPENAI_AVAILABLE:
                raise ImportError("openai package requis. Installez avec: pip install openai")
            self.client = openai.OpenAI(api_key=api_key)
            self.provider = "openai"
        elif "claude" in model_name.lower():
            if not ANTHROPIC_AVAILABLE:
                raise ImportError("anthropic package requis. Installez avec: pip install anthropic")
            self.client = anthropic.Anthropic(api_key=api_key)
            self.provider = "anthropic"
        else:
            logger.warning(f"Mod√®le {model_name} non reconnu, utilisation mode fallback")
            self.client = None
            self.provider = "fallback"
        
        logger.info(f"TweetClassifier initialis√© avec {model_name} (provider: {self.provider})")
    
    def _build_user_prompt(self, tweet: str) -> str:
        """
        Construit le prompt utilisateur avec few-shot examples.
        
        Args:
            tweet: Texte du tweet √† classifier
            
        Returns:
            Prompt format√© avec exemples et tweet √† classifier
        """
        prompt_parts = ["# EXEMPLES DE CLASSIFICATION\n"]
        
        # Ajouter les exemples few-shot
        for i, example in enumerate(self.FEW_SHOT_EXAMPLES, 1):
            prompt_parts.append(f"\n## Exemple {i}")
            prompt_parts.append(f"Tweet: {example['tweet']}")
            prompt_parts.append(f"Classification: {json.dumps(example['classification'], indent=2, ensure_ascii=False)}")
            prompt_parts.append(f"Justification: {example['justification']}\n")
        
        # Ajouter le tweet √† classifier
        prompt_parts.append("\n# TWEET √Ä CLASSIFIER\n")
        prompt_parts.append(f"Tweet: {tweet}")
        prompt_parts.append("\nClassification (JSON uniquement, pas de texte suppl√©mentaire):")
        
        return "\n".join(prompt_parts)
    
    def _call_llm(self, tweet: str) -> Dict[str, Any]:
        """
        Appelle le LLM pour classifier le tweet.
        
        Args:
            tweet: Texte du tweet
            
        Returns:
            R√©ponse JSON du LLM pars√©e
            
        Raises:
            Exception: Si l'appel LLM √©choue
        """
        user_prompt = self._build_user_prompt(tweet)
        
        try:
            if self.provider == "openai":
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": self.SYSTEM_PROMPT},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                    response_format={"type": "json_object"}  # Force JSON
                )
                content = response.choices[0].message.content
                
            elif self.provider == "anthropic":
                response = self.client.messages.create(
                    model=self.model_name,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature,
                    system=self.SYSTEM_PROMPT,
                    messages=[
                        {"role": "user", "content": user_prompt}
                    ]
                )
                content = response.content[0].text
            
            else:
                # Fallback avec classification par r√®gles
                return self._fallback_classification(tweet)
            
            # Parser le JSON
            try:
                result = json.loads(content)
                return result
            except json.JSONDecodeError as e:
                logger.error(f"Erreur parsing JSON: {content[:200]}")
                # Tenter d'extraire le JSON du texte
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group(0))
                raise
                
        except Exception as e:
            logger.error(f"Erreur appel LLM: {e}")
            # Fallback sur classification par r√®gles
            return self._fallback_classification(tweet)
    
    def _fallback_classification(self, tweet: str) -> Dict[str, Any]:
        """
        Classification de fallback bas√©e sur des r√®gles simples.
        
        Utilis√©e si le LLM √©choue ou n'est pas disponible.
        
        Args:
            tweet: Texte du tweet
            
        Returns:
            Classification basique par r√®gles
        """
        tweet_lower = tweet.lower()
        
        # D√©tection r√©clamation (mots-cl√©s n√©gatifs)
        reclamation_keywords = ["probl√®me", "panne", "coup√©", "bug", "erreur", "nul", 
                                "lent", "ne fonctionne", "pas de", "toujours rien"]
        is_reclamation = any(kw in tweet_lower for kw in reclamation_keywords)
        
        # Th√©matique (premier match)
        theme = "AUTRE"
        if any(kw in tweet_lower for kw in ["fibre", "internet", "connexion", "box"]):
            theme = "FIBRE"
        elif any(kw in tweet_lower for kw in ["mobile", "4g", "5g", "forfait", "appel"]):
            theme = "MOBILE"
        elif any(kw in tweet_lower for kw in ["tv", "cha√Æne", "replay", "freebox tv"]):
            theme = "TV"
        elif any(kw in tweet_lower for kw in ["facture", "prix", "paiement", "pr√©l√®vement"]):
            theme = "FACTURE"
        elif any(kw in tweet_lower for kw in ["sav", "service client", "assistance"]):
            theme = "SAV"
        elif any(kw in tweet_lower for kw in ["r√©seau", "couverture", "antenne"]):
            theme = "RESEAU"
        
        # Sentiment
        sentiment = "NEUTRE"
        negative_words = ["nul", "mauvais", "probl√®me", "bug", "col√®re", "marre"]
        positive_words = ["bien", "merci", "super", "excellent", "top"]
        if any(word in tweet_lower for word in negative_words):
            sentiment = "NEGATIF"
        elif any(word in tweet_lower for word in positive_words):
            sentiment = "POSITIF"
        
        # Urgence
        urgence = "FAIBLE"
        if any(kw in tweet_lower for kw in ["urgent", "critique", "coup√©", "plus de"]):
            urgence = "ELEVEE" if is_reclamation else "MOYENNE"
        elif is_reclamation:
            urgence = "MOYENNE"
        
        # Type incident
        type_incident = "INFO"
        if "panne" in tweet_lower or "coup√©" in tweet_lower:
            type_incident = "PANNE"
        elif "lent" in tweet_lower or "lenteur" in tweet_lower:
            type_incident = "LENTEUR"
        elif "facture" in tweet_lower or "prix" in tweet_lower:
            type_incident = "FACTURATION"
        elif "sav" in tweet_lower or "service" in tweet_lower:
            type_incident = "PROCESSUS_SAV"
        elif is_reclamation:
            type_incident = "AUTRE"
        
        return {
            "is_reclamation": "OUI" if is_reclamation else "NON",
            "theme": theme,
            "sentiment": sentiment,
            "urgence": urgence,
            "type_incident": type_incident,
            "confidence": 0.6,  # Confiance mod√©r√©e pour fallback
            "justification": "Classification automatique par r√®gles (fallback)"
        }
    
    def classify(self, tweet: str, tweet_id: Optional[str] = None) -> ClassificationResult:
        """
        Classifie un tweet unique.
        
        Args:
            tweet: Texte du tweet √† classifier
            tweet_id: ID optionnel du tweet
            
        Returns:
            R√©sultat de classification structur√©
            
        Raises:
            ValueError: Si la classification √©choue
        """
        try:
            # Appeler le LLM
            raw_result = self._call_llm(tweet)
            
            # Cr√©er l'objet ClassificationResult avec validation
            result = ClassificationResult(
                is_reclamation=raw_result["is_reclamation"],
                theme=raw_result["theme"],
                sentiment=raw_result["sentiment"],
                urgence=raw_result["urgence"],
                type_incident=raw_result["type_incident"],
                confidence=raw_result.get("confidence", 0.8),
                justification=raw_result.get("justification", "Classification automatique"),
                tweet_id=tweet_id
            )
            
            # Logger les classifications √† faible confiance
            if result.confidence < 0.7:
                logger.warning(
                    f"Classification faible confiance ({result.confidence:.2f}) "
                    f"pour tweet: {tweet[:50]}..."
                )
            
            return result
            
        except Exception as e:
            logger.error(f"Erreur classification tweet: {e}")
            # En cas d'erreur totale, retourner un fallback
            fallback = self._fallback_classification(tweet)
            return ClassificationResult(
                tweet_id=tweet_id,
                **fallback
            )
    
    def batch_classify(
        self,
        tweets: List[str],
        tweet_ids: Optional[List[str]] = None
    ) -> List[ClassificationResult]:
        """
        Classifie un batch de tweets (optimis√© pour performance).
        
        Args:
            tweets: Liste de tweets √† classifier
            tweet_ids: Liste optionnelle d'IDs correspondants
            
        Returns:
            Liste de r√©sultats de classification
        """
        if tweet_ids is None:
            tweet_ids = [None] * len(tweets)
        
        results = []
        total = len(tweets)
        
        logger.info(f"D√©but classification batch de {total} tweets")
        
        for i, (tweet, tweet_id) in enumerate(zip(tweets, tweet_ids), 1):
            try:
                result = self.classify(tweet, tweet_id)
                results.append(result)
                
                if i % 10 == 0:
                    logger.info(f"Progression: {i}/{total} tweets classifi√©s")
                    
            except Exception as e:
                logger.error(f"Erreur tweet {i}: {e}")
                # Ajouter un r√©sultat de fallback
                fallback = self._fallback_classification(tweet)
                results.append(ClassificationResult(tweet_id=tweet_id, **fallback))
        
        logger.info(f"Classification batch termin√©e: {len(results)}/{total} r√©ussis")
        return results
    
    def export_results(
        self,
        results: List[ClassificationResult],
        output_path: str,
        format: str = "json"
    ) -> None:
        """
        Exporte les r√©sultats de classification.
        
        Args:
            results: Liste de r√©sultats
            output_path: Chemin du fichier de sortie
            format: Format d'export (json, csv, excel)
        """
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        if format == "json":
            data = [result.dict() for result in results]
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False, default=str)
        
        elif format in ["csv", "excel"]:
            df = pd.DataFrame([result.dict() for result in results])
            if format == "csv":
                df.to_csv(output_path, index=False, encoding='utf-8')
            else:
                df.to_excel(output_path, index=False)
        
        else:
            raise ValueError(f"Format {format} non support√©. Utilisez json, csv ou excel")
        
        logger.info(f"R√©sultats export√©s vers {output_path} (format: {format})")


# Fonction utilitaire pour usage simple
def classify_tweet(tweet: str, model_name: str = "gpt-4", api_key: Optional[str] = None) -> ClassificationResult:
    """
    Fonction utilitaire pour classifier un tweet rapidement.
    
    Args:
        tweet: Texte du tweet
        model_name: Nom du mod√®le LLM
        api_key: Cl√© API
        
    Returns:
        R√©sultat de classification
        
    Example:
        >>> result = classify_tweet("@Free Ma box ne fonctionne plus !")
        >>> print(result.is_reclamation)
        OUI
    """
    classifier = TweetClassifier(model_name=model_name, api_key=api_key)
    return classifier.classify(tweet)

