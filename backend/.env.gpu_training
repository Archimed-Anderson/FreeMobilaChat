# =============================================================================
# GPU TRAINING CONFIGURATION
# Copy these settings to your main .env file or load this file separately
# =============================================================================

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Model Selection (choose one)
# For RTX 4060 (8GB VRAM) - Recommended models:
TRAINING_MODEL_NAME=camembert-base                    # French BERT (110M params) - Fast training
# TRAINING_MODEL_NAME=microsoft/DialoGPT-medium      # Conversational model (117M params)
# TRAINING_MODEL_NAME=mistralai/Mistral-7B-v0.1      # Requires QLoRA (7B params)

# Quantization Settings (for memory efficiency)
USE_QUANTIZATION=true
QUANTIZATION_BITS=4                                   # 4-bit quantization for memory savings

# LoRA Configuration (Parameter-Efficient Fine-tuning)
USE_LORA=true
LORA_R=16                                            # LoRA rank (higher = more parameters)
LORA_ALPHA=32                                        # LoRA scaling parameter
LORA_DROPOUT=0.1                                     # LoRA dropout rate

# =============================================================================
# TRAINING HYPERPARAMETERS
# =============================================================================

# Learning Rate (critical parameter)
LEARNING_RATE=2e-5                                   # Conservative for fine-tuning
# LEARNING_RATE=5e-5                                 # Alternative for faster convergence

# Training Duration
NUM_EPOCHS=3                                         # Start with 3 epochs to avoid overfitting
WARMUP_STEPS=100                                     # Learning rate warmup

# Batch Size and Memory Management
TRAINING_BATCH_SIZE=2                                # Small batch for RTX 4060
GRADIENT_ACCUMULATION_STEPS=4                        # Effective batch size = 2 * 4 = 8
MAX_GRAD_NORM=1.0                                    # Gradient clipping

# Regularization
WEIGHT_DECAY=0.01                                    # L2 regularization

# =============================================================================
# MEMORY OPTIMIZATION
# =============================================================================

# Memory Saving Techniques
USE_GRADIENT_CHECKPOINTING=true                      # Trade compute for memory
USE_MIXED_PRECISION=true                             # FP16 training for speed/memory
DATALOADER_NUM_WORKERS=0                             # 0 for Windows compatibility

# =============================================================================
# MONITORING AND EVALUATION
# =============================================================================

# Evaluation Frequency
EVAL_STEPS=100                                       # Evaluate every 100 steps
SAVE_STEPS=500                                       # Save checkpoint every 500 steps
LOGGING_STEPS=10                                     # Log every 10 steps

# Early Stopping
EARLY_STOPPING_PATIENCE=3                            # Stop if no improvement for 3 evaluations

# =============================================================================
# PATHS AND DIRECTORIES
# =============================================================================

# Model Output
TRAINING_OUTPUT_DIR=./models/fine_tuned              # Where to save the trained model
TRAINING_CACHE_DIR=./cache/huggingface               # HuggingFace cache directory

# =============================================================================
# HARDWARE-SPECIFIC SETTINGS FOR RTX 4060
# =============================================================================

# Optimal settings for your RTX 4060 (8GB VRAM):
# - Use quantization + LoRA for models > 1B parameters
# - Batch size 1-4 depending on sequence length
# - Gradient accumulation to simulate larger batches
# - Mixed precision training (FP16/BF16)
# - Gradient checkpointing for memory savings

# Expected training times on RTX 4060:
# - CamemBERT (110M): ~30-60 minutes
# - Mistral-7B + QLoRA: ~2-4 hours
# - Dataset size: 3,764 tweets

# =============================================================================
# MONITORING INTEGRATION
# =============================================================================

# Weights & Biases (optional)
# WANDB_PROJECT=freemobilachat-training
# WANDB_ENTITY=your_username

# TensorBoard (enabled by default)
# Logs will be saved to: {TRAINING_OUTPUT_DIR}/logs

# =============================================================================
# ADVANCED CONFIGURATIONS
# =============================================================================

# For larger models (7B+) with QLoRA:
# TRAINING_MODEL_NAME=mistralai/Mistral-7B-v0.1
# USE_QUANTIZATION=true
# QUANTIZATION_BITS=4
# USE_LORA=true
# LORA_R=64
# LORA_ALPHA=128
# TRAINING_BATCH_SIZE=1
# GRADIENT_ACCUMULATION_STEPS=8

# For faster training with smaller models:
# TRAINING_MODEL_NAME=camembert-base
# USE_QUANTIZATION=false
# USE_LORA=false
# TRAINING_BATCH_SIZE=4
# GRADIENT_ACCUMULATION_STEPS=2
# LEARNING_RATE=5e-5
